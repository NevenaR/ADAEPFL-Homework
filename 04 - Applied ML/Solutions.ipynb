{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Panda\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "# our code\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data = pd.read_csv('CrowdstormingDataJuly1st.csv', sep=',', parse_dates=['birthday'])\n",
    "soccer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must understand data we will use and clean them if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed description of each columns is provided in the file DATA.md. We invite the reader to take note of these descriptions before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute age\n",
    "\n",
    "A first modification that we propose to do is to compute the age of players according to the given birthday date. Thus, we'll use this feature if needed instead of the birthday's column (which is quite understandable as we use a random forest where each decision tree will split data according to the values). To keep futur model, we would made, usable with other data, we prefere compute the age with the moment when data has been collected.\n",
    "\n",
    "> Doc compute_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data['age'] = soccer_data.apply(utils.compute_age, axis=1)\n",
    "soccer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge raters' values\n",
    "\n",
    "Here, as we want to determine the skin color of a player according to given data, it is important that at least one rater has given a note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data['rater1'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data['rater2'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is some players for whom there is no rater 1 or rater 2 (in particular, here, it seems that when there is no rater 1, there is no rater 2).\n",
    "\n",
    "We decide to remove all dyad when we don't have any note.\n",
    "\n",
    "Note: We could have chose to drop all rows where there is no photo ID, but it is better to consider directly raters instead, as in theory (!) nothing prevents having a photo ID but for a player but no raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean = soccer_data[soccer_data['rater1'].notnull() | soccer_data['rater2'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> D'après le notebook donné, on fait une moyenne --> Thus, we decide to combine these data to have an unique note. Here, we suppose that raters' votes are independent (no influence on votes between the two raters) and that raters were honest, for lack of exactitude. So, we used the mean to compute this unique note.\n",
    "\n",
    "> On préfère travailler avec des entiers qu'avec des floats d'où le fois 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['rater'] = np.floor(soccer_data_clean[['rater1', 'rater2']].mean(axis=1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rater_distinct_values = soccer_data_clean['rater'].value_counts(dropna=False, sort=False).plot(kind='bar')\n",
    "rater_distinct_values.set_ylabel('Number of rates')\n",
    "rater_distinct_values.set_xlabel('Rate values')\n",
    "rater_distinct_values.set_title('Number of rates by values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage null values\n",
    "\n",
    "Now, let's display if there are any null values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For position, we do nothing at this stage. However, for height and weight, we decide to use mean of values to replace null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['height', 'weight']] = soccer_data_clean[['height', 'weight']].fillna(soccer_data_clean[['height', 'weight']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['position'] = soccer_data_clean['position'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppression de alpha 3, meanIAT et meanExp\n",
    "\n",
    "IAT and Explicit bias scores are very important, so we decide to drop any dyad where these values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean = soccer_data_clean.dropna(axis=0, how='any', subset=['Alpha_3', 'meanIAT', 'meanExp'])\n",
    "soccer_data_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Processing data for machine learning\n",
    "\n",
    "### Manage the dimension of dyad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe all data related to IAT and Explicit bias scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['meanIAT', 'nIAT', 'seIAT', 'meanExp', 'nExp', 'seExp']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Commentaire ici et regarder les plots\n",
    "\n",
    "> As we can see from the min of nExp and nIAT is equal to two, from those row  we can deduce nothing from the IAT who is not very representative of the entire population.\n",
    "We need to take into account those values in the following ponderation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Higher is the standard error, the lesser we can trust the result of the tests.\n",
    "# We reverse the value in order to take in account only the value who have\n",
    "# a high standard error\n",
    "reverse_seIAT = abs(soccer_data_clean['seIAT'] - max(soccer_data_clean['seIAT']))\n",
    "reverse_seExp = abs(soccer_data_clean['seExp'] - max(soccer_data_clean['seExp']))\n",
    "# In order to not penalize one study compared to the other we need to have the same maximum\n",
    "soccer_data_clean['reverse_seIAT'] = reverse_seIAT / max(reverse_seIAT)\n",
    "soccer_data_clean['reverse_seExp'] = reverse_seExp / max(reverse_seExp)\n",
    "\n",
    "# Compute the score of ponderation, taking in account the standard error and the value of \n",
    "# differents test.\n",
    "soccer_data_clean['associationScore'] = (soccer_data_clean['meanIAT']*soccer_data_clean['reverse_seIAT']  +\n",
    "                                         soccer_data_clean['meanExp']*soccer_data_clean['reverse_seExp']) / (2)\n",
    "\n",
    "\n",
    "# Plot the result to show that there are not too much value equals to 0.\n",
    "#soccer_data_clean['reverse_seIAT'].plot()\n",
    "#soccer_data_clean['reverse_seExp'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;text-align:justify;\">To schematize, there are four cases we may consider, for each player, regarding skin color and IAT and Explicit bias scores' influence.\n",
    "\n",
    "1. Referee's country has a positive score (IAT or Explicit bias) and player is black (rate from 0.5 to 1).\n",
    "2. Referee's country has a positive score (IAT or Explicit bias) and player is white (rate 0 to 0.5).\n",
    "3. Referee's country has a negative score (IAT or Explicit bias) and player is black (rate from 0.5 to 1).\n",
    "4. Referee's country has a negative score (IAT or Explicit bias) and player is white (rate from 0 to 0.5).\n",
    "\n",
    "Note: We remind that a positive score for IAT or Explicit bias corresponds to faster white | good, black | bad associations and to greater feelings of warmth toward whites versus blacks (respectively). The countrary is true if score is negative.\n",
    "\n",
    "Now, we must make some assumptions and important decisions.\n",
    "\n",
    "The first case will be the case we'll focus on the most. Indeed, we assume that there are some correlation between the number of red/yellow cards given to a player and the referee's country (and it is basically why IAT and Explicit bias are given here). Thus, in such case, we'll increase number of yellow/red cards to take into account the bias.\n",
    "\n",
    "The other cases are not really interesting. For example, for the second and third cases, we assume here that if a yellow/red card was given, the skin color of the player was not taken into account.\n",
    "\n",
    "We don't deny that it is possible for a referee to not give a yellow/red card even if he must had to, because the player's skin color is the same as the one which is \"favourite\" (second and third cases), or that a referee gave more yellow/red cards to a white player because his \"favourite\" skin color is black (opposite of the first case), but if we also increase the number of cards given it would be difficult to highlight some racism behaviour and to entirely use the number of red/yellow cards (increasing data in these four cases would simply shift values).\n",
    "\n",
    "Note: Our decision is subjective, but describe the most actual problems in soccer (it is more common to have racism with black players than with white players). Also, the major part of referees are from countries where white people are the majority (Europe, North America):\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will increase the number of yellow/red cards iff a player is black, and this for each dyad.\n",
    "\n",
    "> Doc pondered_number_of_cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column_name in ['yellowCards', 'yellowReds', 'redCards']:\n",
    "    soccer_data_clean['pondered' + column_name[0].upper() + column_name[1:]] = soccer_data_clean.apply(func=utils.pondered_number_of_cards, args=(column_name,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['playerShort', 'yellowCards', 'ponderedYellowCards', 'yellowReds', 'ponderedYellowReds', 'redCards', 'ponderedRedCards']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Si c'est possible de faire un graphique de la différence entre notre pondération et les valeurs initiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean.plot.scatter(x='yellowCards', y='ponderedYellowCards');\n",
    "soccer_data_clean.plot.scatter(x='yellowReds', y='ponderedYellowReds');\n",
    "soccer_data_clean.plot.scatter(x='redCards', y='ponderedRedCards');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate by players\n",
    "\n",
    "Then, we sum all the statistics as we want to have one row for each player.\n",
    "\n",
    "> On fait une simple somme pour ces attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_statistics = soccer_data_clean[['playerShort', 'games', 'victories', 'defeats', 'goals', 'ponderedYellowCards', 'ponderedYellowReds', 'ponderedRedCards']].groupby('playerShort').sum()\n",
    "global_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our final DataFrame containing information about a player and some statistics for his career.\n",
    "\n",
    "> On assume que toutes les caractéristiques d'un joueur est la même -> on prend la première ligne\n",
    "\n",
    "> At the end of this part, DataFrame's size was substantially reduced. However, we draw reader's attention on the fact that either we created new features which includes data from previous features (it's the case for the ponderation of cards, which uses IAT and Explicit bias scores for example) or we dropped features which are not useful for what we plan to do (like the photoID or the refNum), so we can safely continue our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players = soccer_data_clean.groupby('playerShort').first()\n",
    "\n",
    "for feature in ['club', 'leagueCountry', 'position']:\n",
    "    global_statistics = global_statistics.merge(pd.get_dummies(players[feature]), left_index=True, right_index=True)\n",
    "\n",
    "soccer_data_all_features = global_statistics.join(players[['age', 'height', 'weight', 'rater']])\n",
    "soccer_data_all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We categorized two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_all_features['raterBinarized'] = pd.cut(soccer_data_all_features['rater'], [0, 51, 101], labels=['light skin', 'dark skin'], right=False)\n",
    "soccer_data_all_features[['raterBinarized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Graphique ici des différentes valeurs, montrer qu'il y a que des blancs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colorSkin = pd.cut(soccer_data_all_features['rater'], [0, 26, 51, 76, 101], labels=['very light skin','light skin','dark skin','very dark skin'], right=False)\n",
    "colorSkin.value_counts().plot(kind='pie', figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - From player description to skin color\n",
    "\n",
    "**Train a sklearn.ensemble.RandomForestClassifier that given a soccer player description outputs his skin color. Show how different parameters passed to the Classifier affect the overfitting issue. Perform cross-validation to mitigate the overfitting of your model. Once you assessed your model, inspect the feature_importances_ attribute and discuss the obtained results. With different assumptions on the data (e.g., dropping certain features even before feeding them to the classifier), can you obtain a substantially different feature_importances_ attribute?**\n",
    "\n",
    "### Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_features = [col for col in soccer_data_all_features.columns if col not in ['rater', 'raterBinarized']]\n",
    "intuitive_usefull_features = ['games', 'victories', 'defeats', 'goals', 'ponderedYellowCards', 'ponderedYellowReds', 'ponderedRedCards', 'age', 'height', 'weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest without cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Doc get_random_forest et run_once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfs = utils.get_random_forests()\n",
    "results_single = utils.run_once(cfs, soccer_data_all_features, all_features, 'raterBinarized')\n",
    "results_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "utils.plot_confusion_matrix(results_single[0]['confusion_matrix'], classes=['dark skin', 'light skin'], title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfs = utils.get_random_forests()\n",
    "results_cv = utils.run_cross_validation(cfs, soccer_data_all_features, all_features, 'raterBinarized')\n",
    "results_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fi = rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "(Plot)\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "(Indexes and names)\n",
    "http://stackoverflow.com/questions/22361781/how-does-sklearn-random-forest-index-feature-importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "soccer_data_splitted = soccer_data_final.copy()\n",
    "soccer_data_splitted['trainingMode'] = np.random.uniform(0, 1, len(soccer_data_final)) <= .75\n",
    "train, test = soccer_data_final[soccer_data_splitted['trainingMode'] == True], soccer_data_splitted[soccer_data_splitted['trainingMode'] == False]\n",
    "\n",
    "y, _ = pd.factorize(train['rater'])\n",
    "rfc.fit(train[features], y)\n",
    "\n",
    "predictions = rfc.predict(test[features])\n",
    "#scores_predictions = rfc.score(predictions, test['rater'])\n",
    "\n",
    "#print(scores_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### SEE PREVIOUS CELL (beginning of \"Cross-validation\" section / SAME CODE\n",
    "\n",
    "n_est = 5\n",
    "result = collections.defaultdict(list)\n",
    "\n",
    "# We loop to find the best parameter for our classifier.\n",
    "\n",
    "# --> les fenetres des valeurs possible doivent étre changé.\n",
    "for n_est in [1,10,100,1000,2000]:\n",
    "    for min_leaf in range(10,11):\n",
    "        for min_split in range (10,11):\n",
    "            \n",
    "            # cross validation using RandomForestClassifier\n",
    "            clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "            scores = cross_val_score(clf, soccer_data_final[features], soccer_data_final['rater'] , cv=10, scoring='accuracy')\n",
    "            \n",
    "            # adding result to the dic.\n",
    "            result['min_leaf'].append(min_leaf)\n",
    "            result['min_split'].append(min_split)\n",
    "            result['n_est'].append(n_est)\n",
    "            result['scores_accuracy'].append(np.mean(scores))\n",
    "            \n",
    "            print('min_leaf: '+str(min_leaf) +\n",
    "                  ' min_split: '+str(min_leaf) +\n",
    "                  ' n_est: '+str(n_est))\n",
    "            print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultDataFrame = pd.DataFrame.from_dict(result)\n",
    "resultDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed_df = resultDataFrame.set_index(['n_est', 'min_leaf','min_split'])\n",
    "indexed_df.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "http://blog.yhat.com/posts/random-forests-in-python.html\n",
    "https://www.dataquest.io/blog/machine-learning-python/\n",
    "https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests\n",
    "\n",
    "http://datascience.stackexchange.com/questions/5226/strings-as-features-in-decision-tree-random-forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doCrossValidation(dataDataframe): \n",
    "    \n",
    "    result = collections.defaultdict(list)\n",
    "\n",
    "    # We loop to find the best parameter for our classifier.\n",
    "\n",
    "    # --> les fenetres des valeurs possible doivent étre changé.\n",
    "    for n_est in [1,10,100,1000]:\n",
    "        for min_leaf in range(10,11):\n",
    "            for min_split in range (10,11):\n",
    "\n",
    "                # cross validation using RandomForestClassifier\n",
    "                clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "                scores = cross_val_score(clf, dataDataframe[features], dataDataframe['rater'] , cv=10, scoring='accuracy')\n",
    "\n",
    "                # adding result to the dic.\n",
    "                result['min_leaf'].append(min_leaf)\n",
    "                result['min_split'].append(min_split)\n",
    "                result['n_est'].append(n_est)\n",
    "                result['scores_accuracy'].append(np.mean(scores))\n",
    "\n",
    "                print('min_leaf: '+str(min_leaf) +\n",
    "                      ' min_split: '+str(min_leaf) +\n",
    "                      ' n_est: '+str(n_est))\n",
    "                print(np.mean(scores))\n",
    "                \n",
    "    resultDataFrame = pd.DataFrame.from_dict(result)\n",
    "    resultDataFrame.head()\n",
    "    indexed_df = resultDataFrame.set_index(['n_est', 'min_leaf','min_split'])\n",
    "    indexed_df.plot(kind='line')\n",
    "    return indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doCrossValidation(soccer_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doCrossValidation(soccer_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#soccer_data_clean.drop('birthday', axis=1, inplace=True)\n",
    "#soccer_data_clean.drop('rater1', axis=1, inplace=True)\n",
    "#soccer_data_clean.drop('rater2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier que les notes pour la couleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rfc = RandomForestClassifier()\n",
    "\n",
    "x = soccer_data_clean[['games','victories','ties','defeats','goals','yellowCards','yellowReds','redCards','age']]\n",
    "y = soccer_data_clean['rater']\n",
    "\n",
    "#scores = cross_val_score(rfc, x, y, cv=10, scoring='accuracy')\n",
    "n_est = 10\n",
    "result = collections.defaultdict(list)\n",
    "\n",
    "#rfc = RandomForestClassifier()\n",
    "\n",
    "#x = soccer_data_clean[['club','leagueCountry','height','weight','position','games','victories','ties','defeats','goals','yellowCards','yellowReds','redCards','refNum','refCountry','Alpha_3','meanIAT','nIAT','seIAT','meanExp','nExp','seExp','age']]\n",
    "#y = soccer_data_clean['rater']\n",
    "\n",
    "#scores = cross_val_score(rfc, x, y, cv=10, scoring='accuracy')\n",
    "#print(scores)\n",
    "\n",
    "#rfc.fit(x, y)\n",
    "#rfc.predict([23, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
