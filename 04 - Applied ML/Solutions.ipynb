{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Panda\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Utils\n",
    "import collections\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "\n",
    "# sickit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# our code\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data = pd.read_csv('CrowdstormingDataJuly1st.csv', sep=',', parse_dates=['birthday'])\n",
    "soccer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must understand data we will use and clean them if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed description of each columns is provided in the file DATA.md. We invite the reader to take note of these descriptions before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute age\n",
    "\n",
    "A first modification that we propose to do is to compute the age of players according to the given birthday date. Thus, we'll use this feature if needed instead of the birthday's column (which is quite understandable as we use a random forest where each decision tree will split data according to the values). To keep futur model, we would made, usable with other data, we prefere compute the age with the moment when data has been collected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data['age'] = soccer_data.apply(utils.compute_age, axis=1)\n",
    "soccer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge raters' values\n",
    "\n",
    "Here, as we want to determine the skin color of a player according to given data, it is important that at least one rater has given a note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data['rater1'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data['rater2'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is some players for whom there is no rater 1 or rater 2 (in particular, here, it seems that when there is no rater 1, there is no rater 2).\n",
    "\n",
    "We decide to remove all dyad when we don't have any note.\n",
    "\n",
    "Note: We could have chose to drop all rows where there is no photo ID, but it is better to consider directly raters instead, as in theory (!) nothing prevents having a photo ID but for a player but no raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean = soccer_data[soccer_data['rater1'].notnull() | soccer_data['rater2'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> D'après le notebook donné, on fait une moyenne --> Thus, we decide to combine these data to have an unique note. Here, we suppose that raters' votes are independent (no influence on votes between the two raters) and that raters were honest, for lack of exactitude. So, we used the mean to compute this unique note.\n",
    "\n",
    "> On préfère travailler avec des entiers qu'avec des floats d'où le fois 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['rater'] = np.floor(soccer_data_clean[['rater1', 'rater2']].mean(axis=1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rater_distinct_values = soccer_data_clean['rater'].value_counts(dropna=False, sort=False).plot(kind='bar')\n",
    "rater_distinct_values.set_ylabel('Number of rates')\n",
    "rater_distinct_values.set_xlabel('Rate values')\n",
    "rater_distinct_values.set_title('Number of rates by values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage null values\n",
    "\n",
    "Now, let's display if there are any null values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For position, we do nothing at this stage. However, for height and weight, we decide to use mean of values to replace null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['height', 'weight']] = soccer_data_clean[['height', 'weight']].fillna(soccer_data_clean[['height', 'weight']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['position'] = soccer_data_clean['position'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suppression de alpha 3, meanIAT et meanExp\n",
    "\n",
    "IAT and Explicit bias scores are very important, so we decide to drop any dyad where these values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean = soccer_data_clean.dropna(axis=0, how='any', subset=['Alpha_3', 'meanIAT', 'meanExp'])\n",
    "soccer_data_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Processing data for machine learning\n",
    "\n",
    "### Manage the dimension of dyad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe all data related to IAT and Explicit bias scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['meanIAT', 'nIAT', 'seIAT', 'meanExp', 'nExp', 'seExp']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Commentaire ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['associationScore'] = (soccer_data_clean['meanIAT'] + soccer_data_clean['meanExp']) / 2\n",
    "soccer_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;text-align:justify;\">To schematize, there are four cases we may consider, for each player, regarding skin color and IAT and Explicit bias scores' influence.\n",
    "\n",
    "1. Referee's country has a positive score (IAT or Explicit bias) and player is black (rate from 0.5 to 1).\n",
    "2. Referee's country has a positive score (IAT or Explicit bias) and player is white (rate 0 to 0.5).\n",
    "3. Referee's country has a negative score (IAT or Explicit bias) and player is black (rate from 0.5 to 1).\n",
    "4. Referee's country has a negative score (IAT or Explicit bias) and player is white (rate from 0 to 0.5).\n",
    "\n",
    "Note: We remind that a positive score for IAT or Explicit bias corresponds to faster white | good, black | bad associations and to greater feelings of warmth toward whites versus blacks (respectively). The countrary is true if score is negative.\n",
    "\n",
    "Now, we must make some assumptions and important decisions.\n",
    "\n",
    "The first case will be the case we'll focus on the most. Indeed, we assume that there are some correlation between the number of red/yellow cards given to a player and the referee's country (and it is basically why IAT and Explicit bias are given here). Thus, in such case, we'll increase number of yellow/red cards to take into account the bias.\n",
    "\n",
    "The other cases are not really interesting. For example, for the second and third cases, we assume here that if a yellow/red card was given, the skin color of the player was not taken into account.\n",
    "\n",
    "We don't deny that it is possible for a referee to not give a yellow/red card even if he must had to, because the player's skin color is the same as the one which is \"favourite\" (second and third cases), or that a referee gave more yellow/red cards to a white player because his \"favourite\" skin color is black (opposite of the first case), but if we also increase the number of cards given it would be difficult to highlight some racism behaviour and to entirely use the number of red/yellow cards (increasing data in these four cases would simply shift values).\n",
    "\n",
    "Note: Our decision is subjective, but describe the most actual problems in soccer (it is more common to have racism with black players than with white players). Also, the major part of referees are from countries where white people are the majority (Europe, North America):\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will increase the number of yellow/red cards iff a player is black, and this for each dyad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column_name in ['yellowCards', 'yellowReds', 'redCards']:\n",
    "    soccer_data_clean['pondered' + column_name[0].upper() + column_name[1:]] = soccer_data_clean.apply(func=utils.pondered_number_of_cards, args=(column_name,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['playerShort', 'yellowCards', 'ponderedYellowCards', 'yellowReds', 'ponderedYellowReds', 'redCards', 'ponderedRedCards']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Si c'est possible de faire un graphique de la différence entre notre pondération et les valeurs initiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate by players\n",
    "\n",
    "Then, we sum all the statistics as we want to have one row for each player.\n",
    "\n",
    "> On fait une simple somme pour ces attributs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_statistics = soccer_data_clean[['playerShort', 'games', 'victories', 'defeats', 'goals', 'ponderedYellowCards', 'ponderedYellowReds', 'ponderedRedCards']].groupby('playerShort').sum()\n",
    "global_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our final DataFrame containing information about a player and some statistics for his career.\n",
    "\n",
    "> On assume que toutes les caractéristiques d'un joueur est la même -> on prend la première ligne\n",
    "\n",
    "> At the end of this part, DataFrame's size was substantially reduced. However, we draw reader's attention on the fact that either we created new features which includes data from previous features (it's the case for the ponderation of cards, which uses IAT and Explicit bias scores for example) or we dropped features which are not useful for what we plan to do (like the photoID or the refNum), so we can safely continue our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players = soccer_data_clean.groupby('playerShort').first()\n",
    "\n",
    "for feature in ['club', 'leagueCountry', 'position']:\n",
    "    global_statistics = global_statistics.merge(pd.get_dummies(players[feature]), left_index=True, right_index=True)\n",
    "\n",
    "soccer_data_all_features = global_statistics.join(players[['age', 'height', 'weight', 'rater']])\n",
    "soccer_data_all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We categorized in four or two categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_all_features['raterCategorized'] = pd.cut(soccer_data_all_features['rater'], [0, 26, 51, 76, 101], labels=['very light skin', 'light skin', 'dark skin', 'very dark skin'], right=False)\n",
    "soccer_data_all_features['raterBinarized'] = pd.cut(soccer_data_all_features['rater'], [0, 51, 101], labels=['light skin', 'dark skin'], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_all_features[['raterCategorized', 'raterBinarized']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Graphique ici des différentes valeurs, montrer qu'il y a que des blancs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - From player description to skin color\n",
    "\n",
    "**Train a sklearn.ensemble.RandomForestClassifier that given a soccer player description outputs his skin color. Show how different parameters passed to the Classifier affect the overfitting issue. Perform cross-validation to mitigate the overfitting of your model. Once you assessed your model, inspect the feature_importances_ attribute and discuss the obtained results. With different assumptions on the data (e.g., dropping certain features even before feeding them to the classifier), can you obtain a substantially different feature_importances_ attribute?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = [col for col in soccer_data_final.columns if col not in ['rater']]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = soccer_data_final[features]\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(soccer_data_final['rater'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_binarizer = preprocessing.LabelBinarizer()\n",
    "y_test_binary = label_binarizer.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ONE-TIME EXECUTION\n",
    "\n",
    "# Function is defined in sklearn documentation and was slightly modified to fit with our needs and our situation\n",
    "# See: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "predictions = rfc.predict(X_test)\n",
    "y_probabilities = rfc.predict_proba(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=soccer_data_final['rater'].unique(), title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
    "# CHECK FOR CORRECT AVERAGE METHOD... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single fit/predict execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = collections.defaultdict(list)\n",
    "\n",
    "# We loop over different parameters' values to compute some metrics and find a good model\n",
    "for n_est in [1, 10, 100, 1000]:\n",
    "    for min_leaf in range(10,11):\n",
    "        for min_split in range (10,11):\n",
    "            clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "            clf.fit(X_train, y_train)\n",
    "            predictions = clf.predict(X_test)\n",
    "            y_probabilities = clf.predict_proba(X_test)\n",
    "            accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "            precision = metrics.precision_score(y_test, predictions, average='macro')\n",
    "            recall = metrics.precision_score(y_test, predictions, average='macro')\n",
    "            f1 = metrics.f1_score(y_test, predictions, average='macro')\n",
    "            roc_auc_score = metrics.roc_auc_score(y_test_binary, y_probabilities)\n",
    "            \n",
    "            results['min_leaf'].append(min_leaf)\n",
    "            results['min_split'].append(min_split)\n",
    "            results['n_est'].append(n_est)\n",
    "            results['accuracy'].append(accuracy)\n",
    "            results['precision'].append(precision)\n",
    "            results['recall'].append(recall)\n",
    "            results['f1'].append(f1)\n",
    "            results['roc_auc'].append(roc_auc_score)\n",
    "            \n",
    "            print('min_leaf: ' + str(min_leaf) + ' min_split: '+ str(min_leaf) + ' n_est: ' + str(n_est))\n",
    "            print('\\t' + 'accuracy: ' + str(accuracy))\n",
    "            print('\\t' + 'precision: ' + str(precision))\n",
    "            print('\\t' + 'recall: ' + str(recall))\n",
    "            print('\\t' + 'f1: ' + str(f1))\n",
    "            print('\\t' + 'roc_auc: ' + str(roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_binary = label_binarizer.fit_transform(soccer_data_final['rater'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_results = collections.defaultdict(list)\n",
    "\n",
    "# We repeat the iterations but, this time, we use cross-validation\n",
    "for n_est in [1, 10, 100, 1000]:\n",
    "    for min_leaf in range(10,11):\n",
    "        for min_split in range (10,11):\n",
    "            # cross validation using RandomForestClassifier\n",
    "            clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "            cv_accuracy = cross_val_score(clf, soccer_data_final[features], soccer_data_final['rater'], cv=10, scoring='accuracy')\n",
    "            cv_roc_auc = cross_val_score(clf, soccer_data_final[features], y_binary, cv=10, scoring='roc_auc')\n",
    "\n",
    "            # adding result to the dic.\n",
    "            cv_results['min_leaf'].append(min_leaf)\n",
    "            cv_results['min_split'].append(min_split)\n",
    "            cv_results['n_est'].append(n_est)\n",
    "            cv_results['cv_accuracy_mean'].append(np.mean(cv_accuracy))\n",
    "            cv_results['cv_roc_auc_mean'].append(np.mean(cv_roc_auc))\n",
    "            \n",
    "            # Print results\n",
    "            print('min_leaf: ' + str(min_leaf) + ' min_split: '+ str(min_leaf) + ' n_est: ' + str(n_est))\n",
    "            print('\\tAccuracy (mean): ' + str(np.mean(cv_accuracy)))\n",
    "            print('\\tROC AUC (mean): ' + str(np.mean(cv_roc_auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fi = rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "(Plot)\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "(Indexes and names)\n",
    "http://stackoverflow.com/questions/22361781/how-does-sklearn-random-forest-index-feature-importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "soccer_data_splitted = soccer_data_final.copy()\n",
    "soccer_data_splitted['trainingMode'] = np.random.uniform(0, 1, len(soccer_data_final)) <= .75\n",
    "train, test = soccer_data_final[soccer_data_splitted['trainingMode'] == True], soccer_data_splitted[soccer_data_splitted['trainingMode'] == False]\n",
    "\n",
    "y, _ = pd.factorize(train['rater'])\n",
    "rfc.fit(train[features], y)\n",
    "\n",
    "predictions = rfc.predict(test[features])\n",
    "#scores_predictions = rfc.score(predictions, test['rater'])\n",
    "\n",
    "#print(scores_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### SEE PREVIOUS CELL (beginning of \"Cross-validation\" section / SAME CODE\n",
    "\n",
    "n_est = 5\n",
    "result = collections.defaultdict(list)\n",
    "\n",
    "# We loop to find the best parameter for our classifier.\n",
    "\n",
    "# --> les fenetres des valeurs possible doivent étre changé.\n",
    "for n_est in [1,10,100,1000,2000]:\n",
    "    for min_leaf in range(10,11):\n",
    "        for min_split in range (10,11):\n",
    "            \n",
    "            # cross validation using RandomForestClassifier\n",
    "            clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "            scores = cross_val_score(clf, soccer_data_final[features], soccer_data_final['rater'] , cv=10, scoring='accuracy')\n",
    "            \n",
    "            # adding result to the dic.\n",
    "            result['min_leaf'].append(min_leaf)\n",
    "            result['min_split'].append(min_split)\n",
    "            result['n_est'].append(n_est)\n",
    "            result['scores_accuracy'].append(np.mean(scores))\n",
    "            \n",
    "            print('min_leaf: '+str(min_leaf) +\n",
    "                  ' min_split: '+str(min_leaf) +\n",
    "                  ' n_est: '+str(n_est))\n",
    "            print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultDataFrame = pd.DataFrame.from_dict(result)\n",
    "resultDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed_df = resultDataFrame.set_index(['n_est', 'min_leaf','min_split'])\n",
    "indexed_df.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "http://blog.yhat.com/posts/random-forests-in-python.html\n",
    "https://www.dataquest.io/blog/machine-learning-python/\n",
    "https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests\n",
    "\n",
    "http://datascience.stackexchange.com/questions/5226/strings-as-features-in-decision-tree-random-forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doCrossValidation(dataDataframe): \n",
    "    \n",
    "    result = collections.defaultdict(list)\n",
    "\n",
    "    # We loop to find the best parameter for our classifier.\n",
    "\n",
    "    # --> les fenetres des valeurs possible doivent étre changé.\n",
    "    for n_est in [1,10,100,1000]:\n",
    "        for min_leaf in range(10,11):\n",
    "            for min_split in range (10,11):\n",
    "\n",
    "                # cross validation using RandomForestClassifier\n",
    "                clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "                scores = cross_val_score(clf, dataDataframe[features], dataDataframe['rater'] , cv=10, scoring='accuracy')\n",
    "\n",
    "                # adding result to the dic.\n",
    "                result['min_leaf'].append(min_leaf)\n",
    "                result['min_split'].append(min_split)\n",
    "                result['n_est'].append(n_est)\n",
    "                result['scores_accuracy'].append(np.mean(scores))\n",
    "\n",
    "                print('min_leaf: '+str(min_leaf) +\n",
    "                      ' min_split: '+str(min_leaf) +\n",
    "                      ' n_est: '+str(n_est))\n",
    "                print(np.mean(scores))\n",
    "                \n",
    "    resultDataFrame = pd.DataFrame.from_dict(result)\n",
    "    resultDataFrame.head()\n",
    "    indexed_df = resultDataFrame.set_index(['n_est', 'min_leaf','min_split'])\n",
    "    indexed_df.plot(kind='line')\n",
    "    return indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doCrossValidation(soccer_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doCrossValidation(soccer_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#soccer_data_clean.drop('birthday', axis=1, inplace=True)\n",
    "#soccer_data_clean.drop('rater1', axis=1, inplace=True)\n",
    "#soccer_data_clean.drop('rater2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier que les notes pour la couleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rfc = RandomForestClassifier()\n",
    "\n",
    "x = soccer_data_clean[['games','victories','ties','defeats','goals','yellowCards','yellowReds','redCards','age']]\n",
    "y = soccer_data_clean['rater']\n",
    "\n",
    "#scores = cross_val_score(rfc, x, y, cv=10, scoring='accuracy')\n",
    "n_est = 10\n",
    "result = collections.defaultdict(list)\n",
    "\n",
    "#rfc = RandomForestClassifier()\n",
    "\n",
    "#x = soccer_data_clean[['club','leagueCountry','height','weight','position','games','victories','ties','defeats','goals','yellowCards','yellowReds','redCards','refNum','refCountry','Alpha_3','meanIAT','nIAT','seIAT','meanExp','nExp','seExp','age']]\n",
    "#y = soccer_data_clean['rater']\n",
    "\n",
    "#scores = cross_val_score(rfc, x, y, cv=10, scoring='accuracy')\n",
    "#print(scores)\n",
    "\n",
    "#rfc.fit(x, y)\n",
    "#rfc.predict([23, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
