{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - Applied ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Panda\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Utils\n",
    "import collections\n",
    "from pprint import pprint\n",
    "from dateutil import relativedelta\n",
    "from datetime import date\n",
    "import itertools\n",
    "\n",
    "# sickit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data = pd.read_csv('CrowdstormingDataJuly1st.csv', sep=',', parse_dates=['birthday'])\n",
    "soccer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must understand data we will use and clean them if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A detailed description of each columns is provided in the file DATA.md. We invite the reader to take note of these descriptions before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first modification that we propose to do is to compute the age of players according to the given birthday date. Thus, we'll use this feature if needed instead of the birthday's column (which is quite understandable as we use a random forest where each decision tree will split data according to the values). To keep futur model, we would made, usable with other data, we prefere compute the age with the moment when data has been collected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_age(row):\n",
    "    '''\n",
    "    Given a player, function returns the years of player.\n",
    "    \n",
    "    row: Row of the DataFrame, representing a dyad which contains a player.\n",
    "    '''\n",
    "    data_date = date(2013, 1, 1)\n",
    "    delta = relativedelta.relativedelta(data_date, row['birthday'])\n",
    "    return delta.years\n",
    "\n",
    "soccer_data['age'] = soccer_data.apply(compute_age, axis=1)\n",
    "soccer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, as we want to determine the skin color of a player according to given data, it is important that at least one rater has given a note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data['rater1'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data['rater2'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is some players for whom there is no rater 1 or rater 2 (in particular, here, it seems that when there is no rater 1, there is no rater 2).\n",
    "\n",
    "We decide to remove all dyad when we don't have any note.\n",
    "\n",
    "Note: We could have chose to drop all rows where there is no photo ID, but it is better to consider directly raters instead, as in theory (!) nothing prevents having a photo ID but for a player but no raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean = soccer_data[soccer_data['rater1'].notnull() | soccer_data['rater2'].notnull()].copy()\n",
    "soccer_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['rater2'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['rater1'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, votes are quite often different between the two raters. Thus, we decide to combine these data to have an unique note.\n",
    "\n",
    "Here, we suppose that raters' votes are independent (no influence on votes between the two raters) and that raters were honest, for lack of exactitude. So, we used the mean to compute this unique note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['rater'] = np.floor(soccer_data_clean[['rater1', 'rater2']].mean(axis=1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['rater'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's display if there are any null values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For position, we do nothing at this stage. However, for height and weight, we decide to use mean of values to replace null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['height', 'weight']] = soccer_data_clean[['height', 'weight']].fillna(soccer_data_clean[['height', 'weight']].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IAT and Explicit bias scores are very important, so we decide to drop any dyad where these values are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soccer_data_clean = soccer_data_clean[soccer_data_clean['meanIAT'].notnull() & soccer_data_clean['meanExp'].notnull()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's describe all data related to IAT and Explicit bias scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['meanIAT', 'nIAT', 'seIAT', 'meanExp', 'nExp', 'seExp']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_clean['associationScore'] = (soccer_data_clean['meanIAT'] + soccer_data_clean['meanExp']) / 2\n",
    "soccer_data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:red;\">To schematize, there are four cases we may consider, for each player, regarding skin color and IAT and Explicit bias scores' influence.\n",
    "\n",
    "1. Referee's country has a positive score (IAT or Explicit bias) and player is black (rate from 0.5 to 1).\n",
    "2. Referee's country has a positive score (IAT or Explicit bias) and player is white (rate 0 to 0.5).\n",
    "3. Referee's country has a negative score (IAT or Explicit bias) and player is black (rate from 0.5 to 1).\n",
    "4. Referee's country has a negative score (IAT or Explicit bias) and player is white (rate from 0 to 0.5).\n",
    "\n",
    "Note: We remind that a positive score for IAT or Explicit bias corresponds to faster white | good, black | bad associations and to greater feelings of warmth toward whites versus blacks (respectively). The countrary is true if score is negative.\n",
    "\n",
    "Now, we must make some assumptions and important decisions.\n",
    "\n",
    "The first case will be the case we'll focus on the most. Indeed, we assume that there are some correlation between the number of red/yellow cards given to a player and the referee's country (and it is basically why IAT and Explicit bias are given here). Thus, in such case, we'll increase number of yellow/red cards to take into account the bias.\n",
    "\n",
    "The other cases are not really interesting. For example, for the second and third cases, we assume here that if a yellow/red card was given, the skin color of the player was not taken into account.\n",
    "\n",
    "We don't deny that it is possible for a referee to not give a yellow/red card even if he must had to, because the player's skin color is the same as the one which is \"favourite\" (second and third cases), or that a referee gave more yellow/red cards to a white player because his \"favourite\" skin color is black (opposite of the first case), but if we also increase the number of cards given it would be difficult to highlight some racism behaviour and to entirely use the number of red/yellow cards (increasing data in these four cases would simply shift values).\n",
    "\n",
    "Note: Our decision is subjective, but describe the most actual problems in soccer (it is more common to have racism with black players than with white players). Also, the major part of referees are from countries where white people are the majority (Europe, North America):\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['refNum', 'Alpha_3']].drop_duplicates('refNum')['Alpha_3'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function that will increase the number of yellow/red cards iff a player is black, and this for each dyad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pondered_number_of_cards(row, cardsName):\n",
    "    '''\n",
    "    Given a player, function analyzes the skin color and ponderates the number of received cards if player is black.\n",
    "    \n",
    "    row: Row of the DataFrame, representing a dyad which contains a player.\n",
    "    cardsName: Type of received cards for the player\n",
    "    '''\n",
    "\n",
    "    nbCards = row[cardsName]\n",
    "    \n",
    "    if row['associationScore'] > 0:\n",
    "        coef = (row['rater'] / 100) * row['associationScore']\n",
    "    elif row['associationScore'] < 0:\n",
    "        coef = (1 - (row['rater'] / 100) ) * row['associationScore']\n",
    "    else:\n",
    "        coef = 0\n",
    "\n",
    "    nbCards += nbCards * coef\n",
    "\n",
    "    return nbCards\n",
    "\n",
    "soccer_data_clean['ponderedYellowCards'] = soccer_data_clean.apply(func=pondered_number_of_cards, args=('yellowCards',), axis=1)\n",
    "soccer_data_clean['ponderedYellowReds'] = soccer_data_clean.apply(func=pondered_number_of_cards, args=('yellowReds',), axis=1)\n",
    "soccer_data_clean['ponderedRedCards'] = soccer_data_clean.apply(func=pondered_number_of_cards, args=('redCards',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soccer_data_clean[['yellowCards', 'yellowReds', 'redCards', 'ponderedYellowCards', 'ponderedYellowReds', 'ponderedRedCards']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we sum all the statistics as we want to have one row for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_statistics = soccer_data_clean[['playerShort', 'games', 'victories', 'defeats', 'goals', 'ponderedYellowCards', 'ponderedYellowReds', 'ponderedRedCards']].groupby('playerShort').sum()\n",
    "global_statistics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create our final DataFrame containing information about a player and some statistics for his career."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players = soccer_data_clean.groupby('playerShort').first()\n",
    "soccer_data_final = global_statistics.join(players[['age', 'height', 'weight', 'rater']])\n",
    "\n",
    "for feature in ['club', 'leagueCountry', 'position']:\n",
    "    global_statistics = global_statistics.merge(pd.get_dummies(players[feature]), left_index=True, right_index=True)\n",
    "\n",
    "soccer_data_final_all_features = global_statistics.join(players[['age', 'height', 'weight', 'rater']])\n",
    "soccer_data_final_all_features.head()\n",
    "\n",
    "#soccer_data_final = global_statistics.join(players[['age', 'height', 'weight', 'rater']])\n",
    "#soccer_data_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note:\n",
    "\n",
    "At the end of this part, DataFrame's size was substantially reduced. However, we draw reader's attention on the fact that either we created new features which includes data from previous features (it's the case for the ponderation of cards, which uses IAT and Explicit bias scores for example) or we dropped features which are not useful for what we plan to do (like the photoID or the refNum), so we can safely continue our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - From player description to skin color\n",
    "\n",
    "**Train a sklearn.ensemble.RandomForestClassifier that given a soccer player description outputs his skin color. Show how different parameters passed to the Classifier affect the overfitting issue. Perform cross-validation to mitigate the overfitting of your model. Once you assessed your model, inspect the feature_importances_ attribute and discuss the obtained results. With different assumptions on the data (e.g., dropping certain features even before feeding them to the classifier), can you obtain a substantially different feature_importances_ attribute?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use categorical data for skin color as it will be the feature used as output here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_final['rater'] = pd.cut(soccer_data_final['rater'], [0, 26, 51, 76, 101], labels=['very light skin','light skin','dark skin','very dark skin'], right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "soccer_data_final['rater'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(See useful links below for source.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = [col for col in soccer_data_final.columns if col not in ['rater']]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = soccer_data_final[features]\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "y = label_encoder.fit_transform(soccer_data_final['rater'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_binarizer = preprocessing.LabelBinarizer()\n",
    "y_test_binary = label_binarizer.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ONE-TIME EXECUTION\n",
    "\n",
    "# Function is defined in sklearn documentation and was slightly modified to fit with our needs and our situation\n",
    "# See: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "predictions = rfc.predict(X_test)\n",
    "y_probabilities = rfc.predict_proba(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=soccer_data_final['rater'].unique(), title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
    "# CHECK FOR CORRECT AVERAGE METHOD... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single fit/predict execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = collections.defaultdict(list)\n",
    "\n",
    "# We loop over different parameters' values to compute some metrics and find a good model\n",
    "for n_est in [1, 10, 100, 1000]:\n",
    "    for min_leaf in range(10,11):\n",
    "        for min_split in range (10,11):\n",
    "            clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "            clf.fit(X_train, y_train)\n",
    "            predictions = clf.predict(X_test)\n",
    "            y_probabilities = clf.predict_proba(X_test)\n",
    "            accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "            precision = metrics.precision_score(y_test, predictions, average='macro')\n",
    "            recall = metrics.precision_score(y_test, predictions, average='macro')\n",
    "            f1 = metrics.f1_score(y_test, predictions, average='macro')\n",
    "            roc_auc_score = metrics.roc_auc_score(y_test_binary, y_probabilities)\n",
    "            \n",
    "            results['min_leaf'].append(min_leaf)\n",
    "            results['min_split'].append(min_split)\n",
    "            results['n_est'].append(n_est)\n",
    "            results['accuracy'].append(accuracy)\n",
    "            results['precision'].append(precision)\n",
    "            results['recall'].append(recall)\n",
    "            results['f1'].append(f1)\n",
    "            results['roc_auc'].append(roc_auc_score)\n",
    "            \n",
    "            print('min_leaf: ' + str(min_leaf) + ' min_split: '+ str(min_leaf) + ' n_est: ' + str(n_est))\n",
    "            print('\\t' + 'accuracy: ' + str(accuracy))\n",
    "            print('\\t' + 'precision: ' + str(precision))\n",
    "            print('\\t' + 'recall: ' + str(recall))\n",
    "            print('\\t' + 'f1: ' + str(f1))\n",
    "            print('\\t' + 'roc_auc: ' + str(roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_binary = label_binarizer.fit_transform(soccer_data_final['rater'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_results = collections.defaultdict(list)\n",
    "\n",
    "# We repeat the iterations but, this time, we use cross-validation\n",
    "for n_est in [1, 10, 100, 1000]:\n",
    "    for min_leaf in range(10,11):\n",
    "        for min_split in range (10,11):\n",
    "            # cross validation using RandomForestClassifier\n",
    "            clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "            cv_accuracy = cross_val_score(clf, soccer_data_final[features], soccer_data_final['rater'], cv=10, scoring='accuracy')\n",
    "            cv_roc_auc = cross_val_score(clf, soccer_data_final[features], y_binary, cv=10, scoring='roc_auc')\n",
    "\n",
    "            # adding result to the dic.\n",
    "            cv_results['min_leaf'].append(min_leaf)\n",
    "            cv_results['min_split'].append(min_split)\n",
    "            cv_results['n_est'].append(n_est)\n",
    "            cv_results['cv_accuracy_mean'].append(np.mean(cv_accuracy))\n",
    "            cv_results['cv_roc_auc_mean'].append(np.mean(cv_roc_auc))\n",
    "            \n",
    "            # Print results\n",
    "            print('min_leaf: ' + str(min_leaf) + ' min_split: '+ str(min_leaf) + ' n_est: ' + str(n_est))\n",
    "            print('\\tAccuracy (mean): ' + str(np.mean(cv_accuracy)))\n",
    "            print('\\tROC AUC (mean): ' + str(np.mean(cv_roc_auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fi = rfc.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "(Plot)\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "(Indexes and names)\n",
    "http://stackoverflow.com/questions/22361781/how-does-sklearn-random-forest-index-feature-importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "soccer_data_splitted = soccer_data_final.copy()\n",
    "soccer_data_splitted['trainingMode'] = np.random.uniform(0, 1, len(soccer_data_final)) <= .75\n",
    "train, test = soccer_data_final[soccer_data_splitted['trainingMode'] == True], soccer_data_splitted[soccer_data_splitted['trainingMode'] == False]\n",
    "\n",
    "y, _ = pd.factorize(train['rater'])\n",
    "rfc.fit(train[features], y)\n",
    "\n",
    "predictions = rfc.predict(test[features])\n",
    "#scores_predictions = rfc.score(predictions, test['rater'])\n",
    "\n",
    "#print(scores_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### SEE PREVIOUS CELL (beginning of \"Cross-validation\" section / SAME CODE\n",
    "\n",
    "n_est = 5\n",
    "result = collections.defaultdict(list)\n",
    "\n",
    "# We loop to find the best parameter for our classifier.\n",
    "\n",
    "# --> les fenetres des valeurs possible doivent étre changé.\n",
    "for n_est in [1,10,100,1000,2000]:\n",
    "    for min_leaf in range(10,11):\n",
    "        for min_split in range (10,11):\n",
    "            \n",
    "            # cross validation using RandomForestClassifier\n",
    "            clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "            scores = cross_val_score(clf, soccer_data_final[features], soccer_data_final['rater'] , cv=10, scoring='accuracy')\n",
    "            \n",
    "            # adding result to the dic.\n",
    "            result['min_leaf'].append(min_leaf)\n",
    "            result['min_split'].append(min_split)\n",
    "            result['n_est'].append(n_est)\n",
    "            result['scores_accuracy'].append(np.mean(scores))\n",
    "            \n",
    "            print('min_leaf: '+str(min_leaf) +\n",
    "                  ' min_split: '+str(min_leaf) +\n",
    "                  ' n_est: '+str(n_est))\n",
    "            print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultDataFrame = pd.DataFrame.from_dict(result)\n",
    "resultDataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexed_df = resultDataFrame.set_index(['n_est', 'min_leaf','min_split'])\n",
    "indexed_df.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links:\n",
    "\n",
    "http://blog.yhat.com/posts/random-forests-in-python.html\n",
    "https://www.dataquest.io/blog/machine-learning-python/\n",
    "https://www.kaggle.com/c/titanic/details/getting-started-with-random-forests\n",
    "\n",
    "http://datascience.stackexchange.com/questions/5226/strings-as-features-in-decision-tree-random-forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def doCrossValidation(dataDataframe): \n",
    "    \n",
    "    result = collections.defaultdict(list)\n",
    "\n",
    "    # We loop to find the best parameter for our classifier.\n",
    "\n",
    "    # --> les fenetres des valeurs possible doivent étre changé.\n",
    "    for n_est in [1,10,100,1000]:\n",
    "        for min_leaf in range(10,11):\n",
    "            for min_split in range (10,11):\n",
    "\n",
    "                # cross validation using RandomForestClassifier\n",
    "                clf = RandomForestClassifier(n_jobs=-1, n_estimators=n_est, min_samples_leaf=min_leaf, min_samples_split=min_split)\n",
    "                scores = cross_val_score(clf, dataDataframe[features], dataDataframe['rater'] , cv=10, scoring='accuracy')\n",
    "\n",
    "                # adding result to the dic.\n",
    "                result['min_leaf'].append(min_leaf)\n",
    "                result['min_split'].append(min_split)\n",
    "                result['n_est'].append(n_est)\n",
    "                result['scores_accuracy'].append(np.mean(scores))\n",
    "\n",
    "                print('min_leaf: '+str(min_leaf) +\n",
    "                      ' min_split: '+str(min_leaf) +\n",
    "                      ' n_est: '+str(n_est))\n",
    "                print(np.mean(scores))\n",
    "                \n",
    "    resultDataFrame = pd.DataFrame.from_dict(result)\n",
    "    resultDataFrame.head()\n",
    "    indexed_df = resultDataFrame.set_index(['n_est', 'min_leaf','min_split'])\n",
    "    indexed_df.plot(kind='line')\n",
    "    return indexed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doCrossValidation(soccer_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doCrossValidation(soccer_data_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#soccer_data_clean.drop('birthday', axis=1, inplace=True)\n",
    "#soccer_data_clean.drop('rater1', axis=1, inplace=True)\n",
    "#soccer_data_clean.drop('rater2', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Vérifier que les notes pour la couleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rfc = RandomForestClassifier()\n",
    "\n",
    "x = soccer_data_clean[['games','victories','ties','defeats','goals','yellowCards','yellowReds','redCards','age']]\n",
    "y = soccer_data_clean['rater']\n",
    "\n",
    "#scores = cross_val_score(rfc, x, y, cv=10, scoring='accuracy')\n",
    "n_est = 10\n",
    "result = collections.defaultdict(list)\n",
    "\n",
    "#rfc = RandomForestClassifier()\n",
    "\n",
    "#x = soccer_data_clean[['club','leagueCountry','height','weight','position','games','victories','ties','defeats','goals','yellowCards','yellowReds','redCards','refNum','refCountry','Alpha_3','meanIAT','nIAT','seIAT','meanExp','nExp','seExp','age']]\n",
    "#y = soccer_data_clean['rater']\n",
    "\n",
    "#scores = cross_val_score(rfc, x, y, cv=10, scoring='accuracy')\n",
    "#print(scores)\n",
    "\n",
    "#rfc.fit(x, y)\n",
    "#rfc.predict([23, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
