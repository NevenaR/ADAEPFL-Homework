{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Interactive Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bfs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil import relativedelta\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import json\n",
    "from pprint import pprint\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "from geopy.geocoders import GeoNames, Nominatim, GoogleV3\n",
    "import locale\n",
    "import seaborn as sns\n",
    "\n",
    "topojson_cantons = r'ch-cantons.topojson.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note for the reader: All cells of the Notebook have been executed. However, the entire Notebook can be executed again. In order to do so, please make sure that you have all the libraries installed. If it is not the case, you have to install them manually.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swiss National Science Foundation is a private and independent organisation in charge of repartition of funds toward the different universities, research centers, laboraties, and so on.\n",
    "\n",
    "In compliance with regulations and for the purpose of transparency, SNSF published online all data regarding the projects it has supported. These data, which are keeped up to date, can be found online, on <a href=\"http://p3.snf.ch/Pages/DataAndDocumentation.aspx\">SNSF's official website</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the present Notebook, we are going to manipulate these data and more specifically data regarding Swiss institutions (public as private) to retrieve some interesting facts as allocated amount to the different cantons of the Swiss Confederation.\n",
    "\n",
    "With this aim in mind, we'll firstly gather all data. Then, we will compute the amount of money allowed to different universities. Finally, we'll display results on an interactive map using different libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary analysis and familiarisation with libraries (test map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before manipulating data, and as proposed in instructions, we propose to familiarise with tools we will use in the present Notebook. In particular, we propose to use provided <a href=\"https://github.com/mbostock/topojson\">TopoJSON</a> (map of Switzerland divided by cantons) to display it with <a href=\"https://github.com/python-visualization/folium\">Folium</a>.\n",
    "\n",
    "The intermediate visualisation consists of displaying Swiss cantons according to amount of money (let's say in million) they received during the past years.\n",
    "\n",
    "Note: Of course, here we use randomly-generated approved amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first retrieve all information of TopoJSON\n",
    "with open(topojson_cantons) as data_file:    \n",
    "    ch_cantons_json = json.load(data_file)\n",
    "\n",
    "# We loop on the JSON file in order to get the initials of each canton.\n",
    "# Moreover, we associate a random number to each canton.\n",
    "cantons_id = [canton['id'] for canton in ch_cantons_json['objects']['cantons']['geometries']]\n",
    "data_test = pd.DataFrame({\n",
    "        'canton': cantons_id,\n",
    "        'value': np.random.randint(0, 100, len(cantons_id))\n",
    "    })\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generated false data, we create the Folium map using provided TopoJSON and the aforesaid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# topojson: Reference to all Swiss cantons\n",
    "# data: Data associated to cantons and used to color the map\n",
    "# threshold_scale: Scale used for data (here, we set a linear scale)\n",
    "map_test = folium.Map(location=[46.8, 8.1], zoom_start=8)\n",
    "map_test.choropleth(\n",
    "    geo_path=topojson_cantons,\n",
    "    topojson='objects.cantons',\n",
    "    data=data_test,\n",
    "    columns=['canton', 'value'],\n",
    "    key_on='feature.id',\n",
    "    fill_color='PuBuGn',\n",
    "    fill_opacity=0.5,\n",
    "    line_opacity=0.7,\n",
    "    threshold_scale=list(np.linspace(data_test['value'].min(), data_test['value'].max(), 6)),\n",
    "    reset=True\n",
    ")\n",
    "map_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNSF data\n",
    "\n",
    "### Grant CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if SNSF provides a consequent set of information about data, in our case, we only use information gathered in *P3_GrantExport.csv* file.\n",
    "\n",
    "Having downloaded the file, we open it with a data parser in order to manipulate date more easily.\n",
    "\n",
    "*Note: The following cell can take some time to run, because of volume of data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snsf_data = pd.read_csv('Data/P3_GrantExport.csv', sep=';', parse_dates=['Start Date', 'End Date'])\n",
    "snsf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of fields and data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before manipulating data, it is important to describe them in order to understand and anticipate possible issues we may encounter.\n",
    "\n",
    "In our case, data are provided by SNSF. As the organisation provides a full description of all fields and in order to avoid overloading Notebook, we redirect the reader to the <a href=\"http://p3.snf.ch/Pages/DataAndDocumentation.aspx\">SNSF's official website</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's display the type of each column to get some intuition about format of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snsf_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we have the Project number and the Discipline Number set as integers while Start Date and End Date are designated as dates, thanks to applied parser.\n",
    "\n",
    "However, some columns draw our attention to some concerns. For example, first column includes quotation marks, which can be problematic during manipulation. Also, some fields have a different type from which expected, like Approved Amount, which is here described as an object instead of an integer. We thus suppose that some data are missing. We'll thus do some further checks about this kind of issues during data handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the first thing we want to do is to set the project number as the index of our dataset. Intuitively, data of this column are unique. And probably, there is no empty values. Let's confirm these facts.\n",
    "\n",
    "*Note: Here, we take advantage of the opportunity to rename the column to get rid of these quotation marks, as they are not useful at all here (quite the reverse).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snsf_data.columns = snsf_data.columns.str.replace(snsf_data.columns[0], 'Project Number')\n",
    "print('Values are unique: %r' % snsf_data['Project Number'].is_unique)\n",
    "print('Number of null values: %d' % sum(pd.isnull(x) for x in snsf_data['Project Number']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there is no duplicates or null values for this field so we set column as index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snsf_data.set_index('Project Number', inplace=True)\n",
    "snsf_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said previously, values of 'Approved Amount' column are objects. Thus, we need to cast them to numeric in order to be able to compute the sum easily.<br/>\n",
    "\n",
    "\n",
    "In our case, it is important to notice that 10910 values cannot be casted to numeric (see below) for two reasons, which are described in official documentation :\n",
    "\n",
    "    - \"This amount is not indicated in the case of mobility fellowships since it depends on administrative factors,\n",
    "    typically the destination, cost of living, family allowances (if applicable) and exchange rate differences.\"\n",
    "    \n",
    "    - \"Also in the case of NCCRs this amount is not available in P3\"\n",
    "     \n",
    " \n",
    "*Note: Regarding the second reason, we made some brief investigation and, <a href=\"http://www.snf.ch/SiteCollectionDocuments/nfs/nccr_guide_2016.pdf\">according to SNSF</a>, NCCR financial helps are attributed to the cantons which receive the most the funds of the organisation. Thus, we can safely conclude that not having these data won't greatly impact our study. Of course, in such situation, it would be preferable to handle data on their globality.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snsf_data['Approved Amount'] = pd.to_numeric(snsf_data['Approved Amount'], errors='coerce')\n",
    "snsf_data['Approved Amount'].value_counts(dropna=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our study, we are interested on manipulating data relative to Swiss public (respectively private) organisations. However, from the documentation, we learn the following statement:\n",
    "\n",
    "*\"This is the institution where the project will largely be carried out according to the application. Pick list. This field is only filled if the research is carried out at a Swiss institution, otherwise the field remains blank. In the case of mobility fellowships, it is generally left empty.\"*\n",
    "\n",
    "For this reason, and from now, we only consider rows (i.e. data) for which university field is filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We only keep rows with 'University' field filled, as they designate Swiss organisations.\n",
    "snsf_data_swiss = snsf_data[snsf_data['University'].notnull()].copy()\n",
    "snsf_data_swiss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Rapid check to ensure that no null values are present in the column anymore\n",
    "sum(pd.isnull(x) for x in snsf_data_swiss['University'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see if the mobility fellowship is really important and if it must be necessarily considered in our study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snsf_data_swiss[snsf_data_swiss['Funding Instrument'].str.contains(\"fellowships\")]['Funding Instrument'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snsf_data_swiss[snsf_data_swiss['Funding Instrument'].str.contains(\"Mobility\")]['Funding Instrument'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the complete list, these specific cases are not substantial.\n",
    "\n",
    "Let's check how many money was attributed for these projects and compare it with total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "swiss_mobility_funds = pd.to_numeric(snsf_data_swiss[['Mobility' in name or 'fellowships' in name for name in snsf_data_swiss['Funding Instrument'].values]]['Approved Amount'], errors='coerce')\n",
    "amount_mobility = swiss_mobility_funds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_swiss_funds = pd.to_numeric(snsf_data_swiss['Approved Amount'], errors='coerce')\n",
    "total_amount = all_swiss_funds.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentage_mobility_over_total = 100*amount_mobility/total_amount\n",
    "percentage_mobility_over_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, amount delivered for Swiss mobility represents only 0.08% of total, so we can safely ignore these rows.\n",
    "\n",
    "*Important note: Here, we are forced to drop these rows as there is no *safe* way to associate a mobility program to an university (and thus to a canton). Considering these data in such situation would be problematic.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Finding location according to universities' names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first important thing we want to do here is to retrieve the location of all universities and organisations.\n",
    "\n",
    "This task represents a lot of work and must be broken down in several steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store the entire list of Swiss universities and organisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating universities' list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "universities_list = snsf_data['University'].dropna().unique()\n",
    "universities_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing fetch of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's next?\n",
    "\n",
    "To retrieve locations, we will use different official API. We do the choice of coding an entire generic function which will manage call to the API and store of the resuts, to ensure reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_location(geolocator, file_path, universities=universities_list):\n",
    "    '''\n",
    "    This functions iterates over a list of place's names and return, for each of entry, exact location using API provided as parameter.\n",
    "    Function also stores the results in a file (here, pickle format is used to compress data).\n",
    "    \n",
    "    geolocator: provided API to retrieve location (Geocoders' instance)\n",
    "    file_path: file where to store the results\n",
    "    universities: list of all place's names for which we want to retrieve the location (here, Swiss universities and organisations)\n",
    "    '''\n",
    "    \n",
    "    # If data were already fetched, we read the file directly\n",
    "    if os.path.exists(file_path):\n",
    "        serie = pd.read_pickle(file_path)\n",
    "    # Else, we use Geolocaters' instance to retrieve locations\n",
    "    else:\n",
    "        match_location = {}\n",
    "        progress_bar = FloatProgress(min=0, max=len(universities))\n",
    "        display(progress_bar)\n",
    "\n",
    "        for university in universities:\n",
    "            match_location[university] = geolocator.geocode(university + ' CH')\n",
    "            progress_bar.value += 1\n",
    "\n",
    "        # At the end of fetch, we store the results in a file\n",
    "        serie = pd.Series(match_location)\n",
    "        serie.to_pickle(file_path)\n",
    "\n",
    "    return serie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE**\n",
    "\n",
    "APIs we use in this Notebook require tokens. Because of privacy and security concerns, the tokens are not listed in the repository.\n",
    "\n",
    "Here, we provide a file which already contains the locations of the universities. However, it is possible for the reader to delete the aforesaid file and force new fetch of data. Note that the reader has then to ensure that he provides a file *env.json* in the repository which contains tokens relatice to the different APIs we use.\n",
    "\n",
    "Format of the file is the following:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"GeoNames-Username\": \"<username>\",\n",
    "  \"GoogleMaps-Key\": \"<token>\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env_file = r'env.json'\n",
    "\n",
    "with open('env.json') as file:    \n",
    "    env = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving locations using GeoNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As proposed in the instructions, we first use GeoNames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_geonames = GeoNames(username=env['GeoNames-Username'])\n",
    "data_location_geonames = find_location(geo_geonames, 'Data/geonmaes_locations.pickle')\n",
    "data_location_geonames.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, GeoNames has only retrieved 5 locations. Thus, 72 locations are still missing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving locations using OpenStreetMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to retrieve more information, and after some search on the Internet, we decide to use OpenStreetMap, because of its popularity and because it is openly licensed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_nominatim = Nominatim()\n",
    "data_location_nominatim = find_location(geo_nominatim, 'Data/nominatim_locations.pickle')\n",
    "data_location_nominatim.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we retrieve 18 universities, which is really better than with previous API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving locations using Google Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we make the decision of using Google Maps to verify if the difference is significant or not. Indeed, Google Maps is an acclaimed service and thus we think that it can be interesting to use it in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geo_google = GoogleV3(api_key=env['GoogleMaps-Key'])\n",
    "data_location_google = find_location(geo_google, 'Data/google_locations.pickle')\n",
    "data_location_google.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we achieved to retrieve 23 universities and organisations' locations, but 55 values are still missing...\n",
    "\n",
    "(We thought that Google Maps would do better, but it is not the case, apparently.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the results are still insufficient after having used three different APIs, we decide to complete missing data manually.\n",
    "\n",
    "*Note: It has been proposed to use different APIs to retrieve understandable names of universities and organisations of the list, in order to improve results when using APIs like Google Maps. For sure, this solution has some benefits, but here, we wanted to cover all parts of the process. As we already used APIs to fetch some data, and as we think that manual work is part of the process sometimes, even if it can be demanding, we are not considering this kind of approach. Also, we think that in this specific case, it would be more time-consuming to implement such a correct process and, at the same time, ensure that we will obtain tangible results is quite unpredictable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before completing manually locations, we merge all the results fetched from the APIs and store them in a single temporary CSV file which thus contains all universities, cantons and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "universities_cantons_geo = pd.DataFrame(snsf_data['University'].dropna().unique(), columns=['University'])  # We retrieve all universities\n",
    "\n",
    "universities_cantons_geo.head()\n",
    "\n",
    "# We convert series created by find_location function into DataFrame containing two columns: University and Location\n",
    "df_location_geonames = pd.DataFrame({'Location':data_location_geonames}).dropna()\n",
    "df_location_geonames.index.name = 'University'\n",
    "df_location_geonames.reset_index(inplace=True)\n",
    "df_location_nominatim = pd.DataFrame({'Location':data_location_nominatim}).dropna()\n",
    "df_location_nominatim.index.name = 'University'\n",
    "df_location_nominatim.reset_index(inplace=True)\n",
    "df_location_google = pd.DataFrame({'Location':data_location_google}).dropna()\n",
    "df_location_google.index.name = 'University'\n",
    "df_location_google.reset_index(inplace=True)\n",
    "\n",
    "def append_coordinates(row, type):\n",
    "    '''\n",
    "    This function checks if a location has been found by a Geolocator for university passed as parameter.\n",
    "    If so, it returns latitude (respectively longitude).\n",
    "    \n",
    "    row: university (containing university name and location, if any)\n",
    "    type: type of coordinate we want to retrieve if a location is set\n",
    "    '''\n",
    "    \n",
    "    results = df_location_geonames[df_location_geonames['University'] == row['University']]['Location']\n",
    "    if results.empty:\n",
    "        results = df_location_nominatim[df_location_nominatim['University'] == row['University']]['Location']\n",
    "    if results.empty:\n",
    "        results = df_location_google[df_location_google['University'] == row['University']]['Location']\n",
    "\n",
    "    if not results.empty:\n",
    "        # First index filter: accessing first result of series 'results'\n",
    "        # Second index filter: accessing the coordinates\n",
    "        # Third index filter: accessing the latitude (respectively longitude)\n",
    "        return results.values[0][1][0 if type == 'Latitude' else 1]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# We apply append_coordinates in order to retrieve latitude and longitude from fetched locations\n",
    "universities_cantons_geo['Latitude'] = universities_cantons_geo.apply(lambda row: append_coordinates(row, 'Latitude'), axis=1)\n",
    "universities_cantons_geo['Longitude'] = universities_cantons_geo.apply(lambda row: append_coordinates(row, 'Longitude'), axis=1)\n",
    "\n",
    "# We save our results in a CSV file\n",
    "universities_cantons_geo.set_index('University').to_csv('Data/universities_cantons_geo_tmp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the process, CSV file contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head Data/universities_cantons_geo_tmp.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating results for all locations (API or manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said, we decide to combine all results from API requests and, for each remaining universities and organisations, we manually search on the web for locations (coordinates) and associated canton.\n",
    "\n",
    "All these results are put in a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_location = pd.read_csv('Data/universities_cantons_geo.csv')\n",
    "match_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During manual research, we found that some values marked as *University* were generic.\n",
    "\n",
    "For instance, we have *NPO (Biblioth., Museen, Verwalt.) - NPO* and *Firmen/Privatwirtschaft - FP*, which denote official and public places and private companies, respectively. In the same way, we have a *Weitere* entry which indicates all other types of organisations and a *Nicht zuteilbar* entry which indicates that there is no assignable value here.\n",
    "\n",
    "One strategy would be to divide the total amount corresponding to these entries and distribute it to all cantons. Let's see in details what are the approved amounts for these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amount_by_university = snsf_data_swiss.groupby('University')['Approved Amount'].sum()\n",
    "amount_by_university[amount_by_university.index.str.contains(\"NPO \\(Biblioth., Museen, Verwalt.\\) - NPO\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_by_university[amount_by_university.index.str.contains(\"Firmen/Privatwirtschaft - FP\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_by_university[amount_by_university.index.str.contains(\"Weitere\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_by_university[amount_by_university.index.str.contains(\"Nicht zuteilbar - NA\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, amounts are quite important here, and thus, using this approach (i.e. divide equally and distribute) can introduce some bias which can be very significant.\n",
    "\n",
    "In absolute terms, we focus on comparison between cantons in our case, so as we add the same value for all cantons (i.e. we do king of translation), it will not have impact. However, the reasoning here is false and can be dangerous. For this reason, we decide to drop these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_location_significant = match_location[match_location['Canton'].notnull()]\n",
    "match_location_significant.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each university, we can compute the sum of 'Approved amount' after having merged the geographic information (cantons) with numeric information (amount). Let's do it, as it is what we are focusing on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_swiss_universities = pd.merge(\n",
    "   snsf_data_swiss[['University', 'Approved Amount']],\n",
    "   match_location_significant[['University', 'Canton']],\n",
    "   on=['University'],\n",
    "   how='inner'\n",
    ")\n",
    "data_swiss_universities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the sum by universities of approved amount, this will be useful to get all money given by an university when we will display the final map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_by_university = data_swiss_universities.groupby('University').sum()\n",
    "amount_by_university.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compute the sum of *Approved amount* for each canton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_by_cantons = data_swiss_universities.groupby('Canton').sum()\n",
    "amount_by_cantons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_by_cantons.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice, the difference between the max and the min is huge.\n",
    "\n",
    "If we take those values, the vizualisation will not be very representative with few colors variations (which is the case when using Folium). Thus, we decide to apply the fourth root on values in order to reduce the gap between the amount of money given to each canton. This transformation is rational here, as we want to do a comparison and not consider each value for each canton apart. An analogy could be done with comparison of signals and the use of logarithm scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "amount_by_cantons['Approved Amount Reduced'] = pow(amount_by_cantons['Approved Amount'], 1/4)\n",
    "amount_by_cantons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cantons_data = pd.DataFrame({\n",
    "        'Canton': cantons_id\n",
    "    })\n",
    "cantons_data.set_index('Canton', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice, some canton have no *Approved Amount* at all... This means that, without considering some specific cases (funds for private companies for example), SNSF has not counted any Swiss project for these cantons over the past years.\n",
    "\n",
    "Thus, we need to add a value (0, in this case) for these cantons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_by_all_cantons = pd.merge(cantons_data, amount_by_cantons, right_index=True, left_index=True, how='left')\n",
    "amount_by_all_cantons.fillna(0, inplace=True)\n",
    "amount_by_all_cantons.reset_index(inplace=True)\n",
    "amount_by_all_cantons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous steps, we created a DataFrame which contains all the information we need in our study. We can now use them, and more precisely, we can create a map with Folium on which we represent the approved amounts for each canton.\n",
    "\n",
    "**Important:** In order to execute the following cell, and in order to print values in correct way, you need to have *fr_CH.utf8* locale for Linux (respectively *fr_CH.UTF-8* locale for OS X/macOS) on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We build the map from the we've got\n",
    "map_final = folium.Map(location=[46.8, 8.1], zoom_start=8)\n",
    "\n",
    "# We set the fr_CH locale to print amounts in correct way\n",
    "try:\n",
    "    locale.setlocale(locale.LC_MONETARY, 'fr_CH.utf8') # Linux locale\n",
    "except:\n",
    "    try:\n",
    "       locale.setlocale(locale.LC_MONETARY, 'fr_CH.UTF-8') # Mac locale\n",
    "    except:\n",
    "        print('Unable to set fr_CH.utf8 or fr_CH.UTF-8 locale. Currency will not be correct.')\n",
    "\n",
    "# We add marker of universities\n",
    "universities_locations = match_location.set_index('University')[['Latitude', 'Longitude']].dropna()\n",
    "for university, row in universities_locations.iterrows():\n",
    "    amount = amount_by_university.loc[university]['Approved Amount']\n",
    "    message = university + ' (' + locale.currency( amount, grouping=True ) + ')'\n",
    "    marker = folium.Marker(row[['Latitude', 'Longitude']], popup=message)\n",
    "    map_final.add_children(marker)\n",
    "    \n",
    "value_column = 'Approved Amount Reduced'\n",
    "\n",
    "# We define the scale (remember that we applied fourth root to amounts)\n",
    "scale = list(\n",
    "    np.linspace(\n",
    "        amount_by_all_cantons[value_column].min(),\n",
    "        amount_by_all_cantons[value_column].max(),\n",
    "        6\n",
    "    )\n",
    ")\n",
    "\n",
    "# We compute choropleth using our dataset\n",
    "map_final.choropleth(\n",
    "    geo_path=topojson_cantons,\n",
    "    data=amount_by_all_cantons,\n",
    "    columns=['Canton', value_column],\n",
    "    key_on='feature.id',\n",
    "    fill_color='PuBu',\n",
    "    fill_opacity=0.8,\n",
    "    line_opacity=0.2,\n",
    "    threshold_scale=scale,\n",
    "    legend_name='Fourth root of approved amount by cantons',\n",
    "    topojson='objects.cantons'\n",
    ")\n",
    "\n",
    "map_final.save('map-approved-amount-by-cantons-scale-1.html')\n",
    "map_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legend\n",
    "\n",
    "For unknown reason (see issue on Slack), legend is not displaying as it should. Thus, here we consider:\n",
    "\n",
    "- colors (fourth root of approved amounts by Swiss cantons) ;\n",
    "- markers (universities (name and amount)).\n",
    "\n",
    "Note that each delimitation represents a canton.\n",
    "\n",
    "We try also with a different scale that use percentules of the data ditribution (we don't add marker this time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_final_percentile_scale = folium.Map(location=[46.8, 8.1], zoom_start=8)\n",
    "\n",
    "value_column = 'Approved Amount'\n",
    "\n",
    "# We define the scale with percentile of the data\n",
    "scale = (\n",
    "    amount_by_all_cantons[value_column].min(),\n",
    "    np.percentile(amount_by_all_cantons[value_column], 20),\n",
    "    np.percentile(amount_by_all_cantons[value_column], 40),\n",
    "    np.percentile(amount_by_all_cantons[value_column], 60),\n",
    "    np.percentile(amount_by_all_cantons[value_column], 80),\n",
    "    amount_by_all_cantons[value_column].max()\n",
    ")\n",
    "\n",
    "# We compute choropleth using our dataset\n",
    "map_final_percentile_scale.choropleth(\n",
    "    geo_path=topojson_cantons,\n",
    "    data=amount_by_all_cantons,\n",
    "    columns=['Canton', value_column],\n",
    "    key_on='feature.id',\n",
    "    fill_color='PuBu',\n",
    "    fill_opacity=0.8,\n",
    "    line_opacity=0.2,\n",
    "    threshold_scale=scale,\n",
    "    legend_name='Fourth root of approved amount by cantons',\n",
    "    topojson='objects.cantons'\n",
    ")\n",
    "\n",
    "map_final_percentile_scale.save('map-approved-amount-by-cantons-scale-2.html')\n",
    "map_final_percentile_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legend\n",
    "\n",
    "- colors (approved amounts by Swiss cantons)\n",
    "\n",
    "The legend of the first map is better because we can easily get approved amount by apply the fourth power   on it. The second map display a bad legend, but we can have more diffences between canton with color, but we can get eh approved amount.\n",
    "\n",
    "### Comment\n",
    "\n",
    "This kind of representation is quite useful here because we can immediately see that some cantons have earned more funds than other. \n",
    "\n",
    "An another observation is that, globally, the more universities and organisations we have in a canton, the higher is the approved amount. There are still some exceptions, and in order to validate such a conclusion, we would prealably do some statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus - Röstigraben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "According to Wikipedia, we can define Röstigraben as follows:\n",
    "\n",
    "*Röstigraben (...) is a humorous term used to refer to the cultural boundary between German-speaking and French-speaking parts of Switzerland.* (<a href=\"https://en.wikipedia.org/wiki/Röstigraben\">source</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, we are interested in showing if there is any significant differences between the different parts of the Switzerland, according to the spoken language of each canton.\n",
    "\n",
    "*Note: Even if originally, Röstigraben consider German-speaking and French-speaking parts of Switzerland, we decide here to include also Italien-speaking and Romansh-speaking parts.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data\n",
    "\n",
    "Prior doing any calculation, we decide to go further on our investigation and we discover an interesting fact: some cantons can have up to 4 official spoken languages (<a href=\"https://en.wikipedia.org/wiki/Cantons_of_Switzerland#List\">source</a>). Also, we see that Röstigraben cuts some cantons in different parts.\n",
    "\n",
    "In our case, we focus on cantons. Also, we do the following assumption: for each canton, we consider the approved amount by dividing it by the number of official spoken languages prior to use it to compute the difference between parts of Switzerland. For example, in Bern, official languages are German and French. Thus, we will consider that Bern is in German-speaking part and its amount is `total_amount_Bern/2`, but, at the same time, we will consider that Bern is in French-speaking part too, and that its respective amount is `total_amount_Bern/2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what we will try to do is to get approved amount by language for Switzerland, then to try to implement an example of analysis regarding the area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we retrieve <a href=\"https://en.wikipedia.org/wiki/Cantons_of_Switzerland#List\">Wikipedia's page content of *Cantons of Switzerland* article</a> in order to get the HTML table and have the relation between cantons and official spoken languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikipedia_page = rq.get('https://en.wikipedia.org/wiki/Cantons_of_Switzerland')\n",
    "wikipedia_soup = bfs(wikipedia_page.text, 'html.parser')\n",
    "\n",
    "# The table we need is the second table of the page.\n",
    "# Not that we can't do simplier, as there is no ID (amongst others)\n",
    "canton_table = pd.read_html(wikipedia_page.text)[1]\n",
    "\n",
    "new_header = canton_table.iloc[0] # We grab first row for the header\n",
    "canton_table = canton_table[1:-1] # We take the data minus the header row\n",
    "canton_table.rename(columns = new_header, inplace=True) # We set header row as the df header\n",
    "\n",
    "# From HTML table, we only get data that we need for the complete dataframe\n",
    "cantons_with_languages = canton_table[['Code', 'Canton', 'Official languages','Area (km2)']].copy()\n",
    "cantons_with_languages.set_index('Code', inplace=True)\n",
    "cantons_with_languages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We append a new column to the DataFrame for each official spoken languages. Value represents a boolean (0=false / 1=true) set to true if the canton speaks th considered language, set to false otherwise.\n",
    "\n",
    "This operation will be useful to groupby languages and additionaly to count the number of spoken languages in the canton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spoken_languages = ['French', 'German', 'Italian', 'Romansh']\n",
    "\n",
    "series = {}\n",
    "for language in spoken_languages:\n",
    "    series[language] = pd.Series(0, index=cantons_with_languages.index)\n",
    "\n",
    "for row in cantons_with_languages.iterrows():\n",
    "    languages = row[1]['Official languages'].split(', ')\n",
    "    for language in languages:\n",
    "        series[language][row[0]] = 1\n",
    "\n",
    "for language, serie in series.items():\n",
    "    cantons_with_languages[language] = serie\n",
    "\n",
    "cantons_with_languages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing data\n",
    "\n",
    "We add the previously-computed amount by canton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cantons_languages_amount = pd.concat([cantons_with_languages, amount_by_all_cantons.set_index('Canton')[['Approved Amount']]], axis=1)\n",
    "cantons_languages_amount.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we compute the number of spoken languages for each canton:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cantons_languages_amount['Number of spoken languages'] = cantons_languages_amount['French'] + cantons_languages_amount['German'] + cantons_languages_amount['Italian'] + cantons_languages_amount['Romansh']\n",
    "cantons_languages_amount.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compute the real amount by language in a canton, according to the assumption we previously made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cantons_languages_amount['Approved Amount per language'] = cantons_languages_amount['Approved Amount'] / cantons_languages_amount['Number of spoken languages']\n",
    "cantons_languages_amount.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This permits to do the sum of approved amount according to the language:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_per_language = []\n",
    "for language in spoken_languages:\n",
    "    total_amount = cantons_languages_amount.groupby(language).sum()['Approved Amount per language'][1]\n",
    "    amount_per_language.append(total_amount)\n",
    "    print('Amount for ' + language + ' cantons: ' + locale.currency( total_amount, grouping=True ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even without any map, we clearly see that there is a huge gap between (French-speaking and German-speaking) parts and (Italian-speaking and Romansh-speaking) parts, under our assumption, and without considering funds for specific cases (as private societes, see previous part). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now, let's see if there is a relation between the approved amount and the size of the area.\n",
    "\n",
    "*Note: Here, we use the same assumption (approved amounts by languages), but here we include the area (km2) variable. Put in other words, if there are two or more official speaking languages for a given canton, we divide equitably the area of the canton by the number of spoken languages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cantons_languages_amount['Area (km2)'] = pd.to_numeric(cantons_languages_amount['Area (km2)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amount_per_area_and_language = []\n",
    "for language in spoken_languages:\n",
    "    total_amount = cantons_languages_amount.groupby(language).sum();\n",
    "    total_per_area  =  total_amount['Approved Amount per language'][1]/ total_amount['Area (km2)'][1]\n",
    "    amount_per_area_and_language.append(total_per_area)\n",
    "    print('Amount per KM2 for ' + language + ' cantons: ' + locale.currency( total_per_area, grouping=True ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, despite the fact that French cantons earn less money in general, concentration of universities allow them to earn more money per km2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some graphs to display these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_graph = pd.DataFrame(data={\n",
    "                             'Cantons': spoken_languages,\n",
    "                             'Accorded amount per language (Mrds Fr.)': amount_per_language,\n",
    "                             'Accorded amount per language and km2 (Fr.)': amount_per_area_and_language\n",
    "                            })\n",
    "data_graph.set_index(['Cantons'], inplace=True)\n",
    "data_graph.plot(kind='bar',y='Accorded amount per language (Mrds Fr.)')\n",
    "data_graph.plot(kind='bar',y='Accorded amount per language and km2 (Fr.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Analysing data using a map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Folium, again, to display our results but on a map, this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We create a set of maps (one map for each language and one global map)\n",
    "map_lang = {\n",
    "    'French': folium.Map(location=[46.8, 8.1], zoom_start=8),\n",
    "    'German': folium.Map(location=[46.8, 8.1], zoom_start=8),\n",
    "    'Italian': folium.Map(location=[46.8, 8.1], zoom_start=8),\n",
    "    'Romansh': folium.Map(location=[46.8, 8.1], zoom_start=8),\n",
    "    'All': folium.Map(location=[46.8, 8.1], zoom_start=8)\n",
    "}\n",
    "\n",
    "map_data_lang = cantons_languages_amount.copy()\n",
    "map_data_lang.reset_index(inplace=True)\n",
    "\n",
    "# We define colors we will use according to the language\n",
    "# Available ones are:\n",
    "# 'BuGn', 'BuPu', 'GnBu', 'OrRd', 'PuBu', 'PuBuGn', 'PuRd', 'RdPu', 'YlGn', 'YlGnBu', 'YlOrBr', and 'YlOrRd'\n",
    "color = {\n",
    "    'French': 'BuGn',\n",
    "    'German': 'BuPu',\n",
    "    'Italian': 'GnBu',\n",
    "    'Romansh': 'PuBu'\n",
    "}\n",
    "\n",
    "# We set the opacity dynamically because we apply multiple choropleth in the map with all languages\n",
    "opacity = [0.9,0.6,0.4,0.2]\n",
    "\n",
    "# For each language\n",
    "for idx, language in enumerate(spoken_languages):\n",
    "\n",
    "    # We use, again, the fourth root\n",
    "    map_data_lang[language] = pow(map_data_lang[language] * map_data_lang['Approved Amount per language'], 1/4)\n",
    "    \n",
    "    # We define the scale for each map\n",
    "    scale = list(\n",
    "        np.linspace(\n",
    "            map_data_lang[language].min(),\n",
    "            map_data_lang[language].max(),\n",
    "            6\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # We use choropleth for each map, using built dataset and other options\n",
    "    map_lang[language].choropleth(\n",
    "        geo_path=topojson_cantons,\n",
    "        data=map_data_lang,\n",
    "        columns=['index', language],\n",
    "        key_on='feature.id',\n",
    "        fill_color=color[language],\n",
    "        fill_opacity=0.7,\n",
    "        line_opacity=0.1,\n",
    "        threshold_scale=scale,\n",
    "        legend_name='Grant money by Swiss ' + language + ' universities',\n",
    "        topojson='objects.cantons'\n",
    "    )\n",
    "    \n",
    "    # We define our global map\n",
    "    map_lang['All'].choropleth(\n",
    "        geo_path=topojson_cantons,\n",
    "        data=map_data_lang,\n",
    "        columns=['index', language],\n",
    "        key_on='feature.id',\n",
    "        fill_color=color[language],\n",
    "        fill_opacity=opacity[idx],\n",
    "        line_opacity=0.1,\n",
    "        threshold_scale=scale,\n",
    "        legend_name='Grant money by Swiss ' + language + ' universities',\n",
    "        topojson='objects.cantons'\n",
    "    )\n",
    "\n",
    "    \n",
    "# Finally, we add markers of universities (only in the global map or if the university is in a canton for which considered language is one of the official languages of the aforesaid canton)\n",
    "universities_locations_canton = match_location_significant.set_index('University').dropna()\n",
    "for university, row in universities_locations_canton.iterrows():\n",
    "    for language, map_single in map_lang.items():\n",
    "        if language == 'All' or cantons_languages_amount.loc[row['Canton']][language] > 0:\n",
    "            amount = amount_by_university.loc[university]['Approved Amount']\n",
    "            message = university + ' - ' + row['Canton'] + ' (' + locale.currency( amount, grouping=True ) + ')'\n",
    "            marker = folium.Marker(row[['Latitude', 'Longitude']], popup=message)\n",
    "            map_single.add_children(marker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legend\n",
    "\n",
    "Again, because of an unknown behavior from Folium (see issue on Slack), we need to define the legend externally. We have, for each map:\n",
    "\n",
    "- colors (fourth root of approved money by Swiss cantons for a given language) ;\n",
    "- markers (universities' names and approved money)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French-speaking part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_lang['French'].save('map-approved-amount-french.html')\n",
    "map_lang['French']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### German-speaking part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_lang['German'].save('map-approved-amount-german.html')\n",
    "map_lang['German']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Italian-speaking part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_lang['Italian'].save('map-approved-amount-italian.html')\n",
    "map_lang['Italian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Romansh-speaking part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_lang['Romansh'].save('map-approved-amount-romansh.html')\n",
    "map_lang['Romansh']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_lang['All'].save('map-approved-amount-all-languages.html')\n",
    "map_lang['All']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: There are obvious limitations here, but there are due to Folium. We would get a better visualisation if opacity could have been better managed by default in Folium.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous maps, we clearly see, again, that there is a huge gap between the different spoken-language parts of Switzerland. Globally, French-speaking and German-speaking parts are better considered in view of approved amounts and fundraisings (which is quite understandable as 1) territories associated to these parts are larger and as 2) the majority of universities and scientific organisations are located in these parts of Switzerland, because of many reasons of different order)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
