{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 05 - Taming text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data processing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from gensim import models, corpora\n",
    "%matplotlib inline\n",
    "\n",
    "# NLTK\n",
    "from nltk.corpus import stopwords, subjectivity\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.data\n",
    "\n",
    "# UTILS\n",
    "import pycountry\n",
    "import random\n",
    "import re\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from os.path import exists\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "# Bonus: NetworkX and Community\n",
    "#import community\n",
    "#import networkx as nx\n",
    "\n",
    "# Our code (autoreload enabled - useful in developement mode)\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** For readability reasons, we decide to store all defined functions in a file called *utils.py*. When functions are used in a cell, a comment mentions it (*# UTILS: use of &lt;name of function 1&gt;[, &lt;name of function 2&gt;, ...]*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some components are missing, please uncomment the following line and execute the cell in order to start the download utility of nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is based on the <a href=\"https://en.wikipedia.org/wiki/Hillary_Clinton_email_controversy\">Hillary Clinton email controversy</a>. Please note that only a fraction of the published emails are used here. The full list, which was published by U.S. Department of State under the Freedom of Information Act, can be consulted <a href=\"https://foia.state.gov/Learn/New.aspx\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits:** Background used in this Notebook was made by <a href=\"http://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> from <a href=\"http://www.flaticon.com\" title=\"Flaticon\">www.flaticon.com</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all data as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aliases = pd.read_csv('hillary-clinton-emails/Aliases.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aliases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_receivers = pd.read_csv('hillary-clinton-emails/EmailReceivers.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_receivers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_emails = pd.read_csv('hillary-clinton-emails/Emails.csv', index_col=0)\n",
    "df_emails.fillna('', inplace=True)\n",
    "df_emails.replace({'\\n': ' '}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_persons = pd.read_csv('hillary-clinton-emails/Persons.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_persons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we retrieve all the content using two columns: *MetadataSubject* and *ExtractedBodyText*. These columns contain respectively the core title and the body of each email in integrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_raw_text\n",
    "emails_content = df_emails['MetadataSubject'] + ' ' + df_emails['ExtractedBodyText']\n",
    "raw_text = utils.generate_raw_text(data=emails_content.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words, which are specific to the study, don't bring anything to our analysis and, in contrary, pollute it. Thus, we decide to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of SPECIFIC_STOP_WORDS\n",
    "for word in utils.SPECIFIC_STOP_WORDS: \n",
    "    raw_text = re.sub(r'\\b%s\\b' % word, '', raw_text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display first word cloud of the raw text, before any processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud = utils.generate_word_cloud(raw_text, file_name='0_raw_text')\n",
    "word_cloud.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - Processing pipeline (cleaning of the text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a word cloud based on the raw corpus -- I recommend you to use the Python word_cloud library. With the help of nltk (already available in your Anaconda environment), implement a standard text pre-processing pipeline (e.g., tokenization, stopword removal, stemming, etc.) and generate a new word cloud. Discuss briefly the pros and cons (if any) of the two word clouds you generated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tokenize the retrieved text. Here, we focus on each word as an all, thanks to a RegExp tokenizer.\n",
    "\n",
    "After the processing, each token will be a candidate for an index entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of REGEX_TOKENIZER, generate_raw_text\n",
    "tokens = utils.REGEX_TOKENIZER.tokenize(raw_text)\n",
    "word_tokenized_text = utils.generate_raw_text(data=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize if there are differences between word clouds describing, respectively, raw text and tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_tokenize = utils.generate_word_cloud(text=word_tokenized_text, file_name='1_word_tokenize')\n",
    "word_cloud_tokenize.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see - and it is not really surprising - there is no significant changes after the tokenization part. Note that using the default tokenizer, we would end up with some words like 'nt' (which come from the decomposition of 'don't' in this case). Using Regexp tokenizer permit us to avoid such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to remove all meaningless words which are present in the emails. Here, we use a set of English words defined as 'stopwords', assuming that all emails were written in such language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_raw_text\n",
    "filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "filtered_text = utils.generate_raw_text(data=filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_stopwords = utils.generate_word_cloud(text=filtered_text, file_name='2_stopwords')\n",
    "word_cloud_stopwords.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even if we don't see big changes, the process removes for sure a lot of unwanted words, that may impact our further analysis if we don't remove them, and even if they don't appear as the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Length before filtering: %d' % len(tokens))\n",
    "print('Length after filtering: %d' % len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize have some disadvantages for multiple reasons: \n",
    "* break up hyphenated sequence\n",
    "* be unsensible to lower case\n",
    "* usability/scalability.\n",
    "\n",
    "This is why, we should use differents method : Lemmatization and stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we tokenized and filtered the content, we may want to apply lemmatization and stemming to obtain the most common words. Indeed, in such text, words are declined in different combinaisons and we must count all of these combinaisons as unique ones for the kind of analysis we run here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: About the difference between stemming and lemmatization**\n",
    "\n",
    "The goal of both processes is to reduce inflectional forms or to find related forms of a word with a common base form ; however, the two techniques differ in the way they achieve to do it. Also, stemming was commonly implemented with reduction techniques, though this is not universal. Lemmatization, as for it, implies a possibly broader scope of functionality, which may include synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firstly reduce inflectional (variant) forms to base form of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#UTILS: use of WORDNET_LEMMATIZER\n",
    "wl_text = utils.WORDNET_LEMMATIZER.lemmatize(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_lemmatizer = utils.generate_word_cloud(text=wl_text, file_name='3_WordNetLemmatizer')\n",
    "word_cloud_lemmatizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use stemmer to reduce terms to their roots before indexing. Note that here we use both Porter stemmer, which is the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of do_stemming_words, PORTER_STEMMER\n",
    "ps_stemming_text = utils.do_stemming_words(stemmer=utils.PORTER_STEMMER, words=wl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_porter_stemmer = utils.generate_word_cloud(text=ps_stemming_text, file_name='4_PorterStemmer')\n",
    "word_cloud_porter_stemmer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for curiosity, we can also use the Snowball stemmer to produce the same result. A good explanation was given about the differences between the main stemmer's algorithms <a href=\"http://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of do_stemming_words, SNOWBALL_STEMMER\n",
    "ss_stemming_text = utils.do_stemming_words(stemmer=utils.SNOWBALL_STEMMER, words=wl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_stemmer = utils.generate_word_cloud(text=ss_stemming_text, file_name='4_SnowballStemmer')\n",
    "word_cloud_stemmer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, no big differences are observed between the two processing (respectively apply of Porter stemmer and Snowball stemmer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Analysis on countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find all the mentions of world countries in the whole corpus, using the pycountry utility (HINT: remember that there will be different surface forms for the same country in the text, e.g., Switzerland, switzerland, CH, etc.) Perform sentiment analysis on every email message using the demo methods in the nltk.sentiment.util module. Aggregate the polarity information of all the emails by country, and plot a histogram (ordered and colored by polarity level) that summarizes the perception of the different countries. Repeat the aggregation + plotting steps using different demo methods from the sentiment analysis module -- can you find substantial differences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we count all the occurrences of countries in Hillary Clinton's emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of count_countries_occurrences\n",
    "countries_occurrences = utils.count_countries_occurrences(ps_stemming_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_countries_occurrences = pd.DataFrame.from_dict(countries_occurrences, orient='index')\n",
    "df_countries_occurrences.columns = ['Occurrences']\n",
    "df_countries_occurrences.sort_values('Occurrences', ascending=False, inplace=True)\n",
    "df_countries_occurrences.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not surprising to find at the top of the list some countries like Israel, Libya, Haiti or even United States. Indeed, these countries are directly linked to the ex-Secretary of State!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the 30 most quoted countries to have a better observation of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of plot_most_quoted_countries\n",
    "utils.plot_most_quoted_countries(df_countries_occurrences, 30)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can remark the fact that the number of quotes of a country drastically fall. We need to pay attention to this detail in the next part of the study as an insufficient number of occurrences could change the sense of the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we want to attribute some kind of sentiment to each quoted country, according of what was discussed in the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_emails_content = df_emails[['MetadataSubject', 'ExtractedBodyText']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use <a href=\"http://sentiwordnet.isti.cnr.it/\">SentiWordNet</a> in order to retrieve the sentimal score of each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UTILS: use of retrieve_email_sentiment\n",
    "results = df_emails_content.apply(utils.retrieve_email_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_plot = pd.DataFrame(pd.value_counts(results['Type']))\n",
    "results_plot.plot(kind='bar', title='Number of occurrences for each type of emails (using SentiWordNet)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice from the previous graph, the number of positive emails is much more greater than the number of the neutral and negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use <a href=\"https://github.com/cjhutto/vaderSentiment\">Vader sentiment analysis</a> to see if we obtain different results.\n",
    "\n",
    "*Note: Some detailed examples on the performance of Vader sentiment analysis used in NLTK can be found <a href=\"http://www.nltk.org/howto/sentiment.html\">here</a>.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of retrieve_email_sentiment\n",
    "results_vader = df_emails_content.apply(utils.retrieve_email_sentiment, args=('Vader',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_vader_plot = pd.DataFrame(pd.value_counts(results_vader['Type']))\n",
    "result_vader_plot.plot(kind='bar', title='Number of occurrences for each type of emails (using Vader)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Vader sentiment analysis, we obtain a different distribution of the data. This difference can be explained by the fact that algorithm associated with SentiWordNet doesn't take into account the entire sentence and only associates sentiment to each word, individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined the sentiment of each email, let's define the sentiment associated to each country. Note that in order to avoid interferences, we only search for exact name of a country. Put in other words, we decide to ignore <a href=\"https://fr.wikipedia.org/wiki/ISO_3166-1_alpha-2\">ISO 3166-1 alpha-2</a> and <a href=\"https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3\">ISO 3166-1 alpha-3</a> country codes, as these codes can represent some words or abbreviations in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of get_countries_sentiment\n",
    "countries_sentiment = utils.get_countries_sentiment(results_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_sentiment = pd.DataFrame.from_dict(countries_sentiment, orient='index')\n",
    "df_countries_sentiment.columns = ['Sentiment']\n",
    "df_countries_sentiment.sort_values('Sentiment', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_sentiment.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_sentiment.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we display the 10 most-preferred and less-preferred countries, respectively, according to sentiment analysis. For some of them, there is explanations about the obtained score, even if it is not evident.\n",
    "\n",
    "For good scores, and as an example, <a href=\"https://en.wikipedia.org/wiki/Haiti%E2%80%93United_States_relations\">relations between Haiti and USA</a> can explain the obtained score. Also, even if USA was directly mobilized as part of Afghanistan's war, it does not mean that the sentiment toward this country is necessarily negative (quite the opposite!).\n",
    "\n",
    "Now, regarding negative scores, some recent events may explain them, as for Libya (see <a href=\"https://en.wikipedia.org/wiki/2012_Benghazi_attack\">2012 Benghazi attack</a> or <a href=\"https://en.wikipedia.org/wiki/Libyan_Civil_War_(2011)\">Libyan Civil War of 2011</a>). Some historical reasons also justify the bad sentiment toward some countries like Serbia (see <a href=\"https://en.wikipedia.org/wiki/Serbia%E2%80%93United_States_relations\">relations between Serbia and USA</a> and <a href=\"https://en.wikipedia.org/wiki/2008_Kosovo_declaration_of_independence\">declaration of independence of Kosovo</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting sentiment and occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge occurrences and sentiment in order to have all the information in an unique DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_data = pd.merge(df_countries_occurrences, df_countries_sentiment, how='inner', left_index=True, right_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sentiment seems to decrease exponentially, we decide to adjust the values using MinMaxScaler. More precisely, we use the log here because of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler((-1, 1))\n",
    "countries_data[['Sentiment']] = np.log(countries_data['Sentiment'] + abs(min(countries_data['Sentiment'])) + 10)\n",
    "countries_data['Sentiment'] = min_max_scaler.fit_transform(countries_data[['Sentiment']].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of plot_countries_by_occurrences_and_sentiment\n",
    "utils.plot_countries_by_occurrences_and_sentiment(countries_data, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Important note: Some results are more difficult to explain and are directly linked with the sentiment analyzers and their bias, but also to the fact that we ignored country codes during the process, while they are used in emails (it is about weighing up the pros and cons here). Also, and among other things, our assumptions and decisions explain the observed differences between different analysis on the same topic (see <a href=\"https://www.kaggle.com/operdeck/d/kaggle/hillary-clinton-emails/hillary-s-sentiment-about-countries\">another example on Kaggle</a>).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also display the countries according to their score. Here, we display the 20 most-preferred countries (left side) and the 20 less-preferred countries (right side). **Here, colors represent the number of occurrences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of plot_sentiment_by_country\n",
    "utils.plot_sentiment_by_country(countries_data, None, nb_country=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that except for few countries (Lativia, Serbia or Lybia), bad feeling seems to be linked with few occurences. Hence, for these countries, we must pay attention and don't make any definitive conclusions!\n",
    "\n",
    "Note that, in contrary, it seems to be a correlation between good sentiment and number of occurrences for a given country, except for Libya..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 - Retrieve of the main topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the models.ldamodel module from the gensim library, run topic modeling over the corpus. Explore different numbers of topics (varying from 5 to 50), and settle for the parameter which returns topics that you consider to be meaningful at first sight.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create our corpus using all content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus, id2word = utils.create_corpus(df_emails['ExtractedBodyText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create our LDA model with different number of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create LDA models with different number of topics and see if we observe significant differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_5_topics = utils.create_lda_model(corpus, id2word, 5)\n",
    "lda_5_topics.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_10_topics = utils.create_lda_model(corpus, id2word, 10)\n",
    "lda_10_topics.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_25_topics = utils.create_lda_model(corpus, id2word, 25)\n",
    "lda_25_topics.print_topics(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_50_topics = utils.create_lda_model(corpus, id2word, 50)\n",
    "lda_50_topics.print_topics(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALTERNATIVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span style=\"color:red\">TODO</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First alternative: use of processed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = utils.SENTENCES_DETECTOR.tokenize(ps_stemming_text.strip())\n",
    "processed_corpus, processed_id2word = utils.create_corpus(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_5_topics_processed = utils.create_lda_model(processed_corpus, processed_id2word, 5)\n",
    "lda_5_topics_processed.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second alternative: regrouping emails by conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:**\n",
    "\n",
    "Here, we clean text. If we want to use raw text and apply only regrouping, replace *CleanedBodyText* by *ExtractedBodyText* in the cells marked as: CELL_1, CELL_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emails_content['CleanedBodyText'] = df_emails_content.apply(utils.process_email_content, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "untitled_emails = df_emails_content[df_emails_content['MetadataSubject'] == '']\n",
    "titled_emails = df_emails_content[df_emails_content['MetadataSubject'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CELL_1\n",
    "aggregated_content = titled_emails.groupby('MetadataSubject').apply(lambda x: \"%s\" % ' '.join(x['CleanedBodyText'])).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CELL_2\n",
    "# NOT OPTIMIZED (2 loops that do the same thing)\n",
    "\n",
    "# To test at the end as alternative for first loop\n",
    "# all_text_array = [[word for word in sentence.lower().split() if word not in stopwords.words('english')] for sentence in utils.SENTENCES_DETECTOR.tokenize(content.strip()) for content in aggregated_content]\n",
    "all_text_array = []\n",
    "all_text_array += utils.get_text_without_Stop_Word(aggregated_content)\n",
    "all_text_array += utils.get_text_without_Stop_Word(untitled_emails['CleanedBodyText'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Will be merged with create_corpus in utils at the end\n",
    "dictionary_conversations = corpora.Dictionary(all_text_array)\n",
    "\n",
    "id2word_conversations = {}\n",
    "for word in dictionary_conversations.token2id:    \n",
    "    id2word_conversations[dictionary_conversations.token2id[word]] = word\n",
    "\n",
    "corpus_conversations = [dictionary_conversations.doc2bow(text) for text in all_text_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_5_topics_conversations = utils.create_lda_model(corpus_conversations, id2word_conversations, 5)\n",
    "lda_5_topics_conversations.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "<b>Build the communication graph (unweighted and undirected) among the different email senders and recipients using the NetworkX library. Find communities in this graph with community.best_partition(G) method from the community detection module. Print the most frequent 20 words used by the email authors of each community. Do these word lists look similar to what you've produced at step 3 with LDA? Can you identify clear discussion topics for each community? Discuss briefly the obtained results.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to build the full graph of all communications. For each email, we add a link between the sender and the receiver (we optionaly add the weight of the link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = defaultdict(lambda : defaultdict(int))\n",
    "nodes = set()\n",
    "for email in df_emails.itertuples():\n",
    "    senderId = email.SenderPersonId\n",
    "    if senderId:\n",
    "        nodes |= {int(senderId)}\n",
    "        receivers = df_email_receivers.loc[df_email_receivers['EmailId'] == email.Index]\n",
    "        for receiver in receivers.itertuples():\n",
    "            nodes |= {int(receiver.PersonId)}\n",
    "            links[int(senderId)][int(receiver.PersonId)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of nodes: ' + str(len(nodes)))\n",
    "print('Number of links: ' + str(sum([len(receivers) for sender, receivers in links.items()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a graphic representation of the graph, we need a simple list of all link. To do so, we flatten the links dictionnary build before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_links = []\n",
    "for sender, receivers in links.items():\n",
    "    for receiver, weight in receivers.items():\n",
    "        list_links.append((sender, receiver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of links: ' + str(len(list_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now, build the first graph with all links and all nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for node in nodes:\n",
    "    name = df_persons.loc[1]['Name']\n",
    "    G.add_node(node, id=node, name=name)\n",
    "G.add_edges_from(list_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nx.draw_circular(G, node_size=30, node_color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we can see, there are a lot of nodes and links and it's not relevant of anything. Let's try with the `community` tools to find partitions and at the same time fetch email for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base on example here: http://perso.crans.org/aynaud/communities/index.html\n",
    "partition = community.best_partition(G)\n",
    "\n",
    "size = float(len(set(partition.values())))\n",
    "pos = nx.spring_layout(G)\n",
    "count = 0\n",
    "communities = {}\n",
    "for communityId in set(partition.values()):\n",
    "    \n",
    "    # Build graph\n",
    "    count = count + 1\n",
    "    list_nodes = [partition_nodes for partition_nodes in partition.keys() if partition[partition_nodes] == communityId]\n",
    "    nx.draw_networkx_nodes(G, pos, list_nodes, node_size=20, node_color=str(count / size))\n",
    "    \n",
    "    # Build communities with emails\n",
    "    content = ''\n",
    "    for personId in list_nodes:\n",
    "        content += ' '.join([email.MetadataSubject + ' ' + email.ExtractedBodyText for email in df_emails.loc[df_emails['SenderPersonId'] == personId].itertuples()]).lower()\n",
    "    communities[communityId] = {\n",
    "        'document': content,\n",
    "        'nodes': list_nodes,\n",
    "        'counter': Counter(content.split())\n",
    "    }\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of communities: ' + str(len(set(partition.values()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that there is only one real community (with Clinton at the middle for sure) and few others without a lot of links. We can get now most used words in each communities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, com in communities.items():\n",
    "    print('Words: ', end='')\n",
    "    for word, number in com['counter'].most_common(20):\n",
    "        print('\"'+ word + '\" (' + str(number) + ')', end=', ')\n",
    "    print('\\n-----------------------')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
