{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 05 - Taming text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Our code (autoreload enabled - useful in developement mode)\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE:** For readability reasons, we decide to store all defined functions in a file called *utils.py*. When functions are used in a cell, a comment mentions it (*# UTILS: use of &lt;name of function 1&gt;[, &lt;name of function 2&gt;, ...]*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If some components are missing, please uncomment the following line and execute the cell in order to start the download utility of nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook is based on the <a href=\"https://en.wikipedia.org/wiki/Hillary_Clinton_email_controversy\">Hillary Clinton email controversy</a>. Please note that only a fraction of the published emails are used here. The full list, which was published by U.S. Department of State under the Freedom of Information Act, can be consulted <a href=\"https://foia.state.gov/Learn/New.aspx\">here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credits:** Background used in this Notebook was made by <a href=\"http://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> from <a href=\"http://www.flaticon.com\" title=\"Flaticon\">www.flaticon.com</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all data as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aliases = pd.read_csv('hillary-clinton-emails/Aliases.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aliases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_receivers = pd.read_csv('hillary-clinton-emails/EmailReceivers.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_receivers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_emails = pd.read_csv('hillary-clinton-emails/Emails.csv', index_col=0)\n",
    "df_emails.fillna('', inplace=True)\n",
    "df_emails.replace({'\\n': ' '}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_persons = pd.read_csv('hillary-clinton-emails/Persons.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_persons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we retrieve all the content using two columns: *MetadataSubject* and *ExtractedBodyText*. These columns contain respectively the core title and the body of each email in integrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emails_content = df_emails['MetadataSubject'] + ' ' + df_emails['ExtractedBodyText']\n",
    "raw_text = ' '.join(emails_content.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display first word cloud of the raw text, before any processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud = utils.generate_word_cloud(raw_text, file_name='0_raw_text')\n",
    "word_cloud.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1 - Processing pipeline (cleaning of the text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate a word cloud based on the raw corpus -- I recommend you to use the Python word_cloud library. With the help of nltk (already available in your Anaconda environment), implement a standard text pre-processing pipeline (e.g., tokenization, stopword removal, stemming, etc.) and generate a new word cloud. Discuss briefly the pros and cons (if any) of the two word clouds you generated.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tokenize the retrieved text. Here, we focus on each word as an all, thanks to a RegExp tokenizer.\n",
    "\n",
    "After the processing, each token will be a candidate for an index entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of REGEX_TOKENIZER\n",
    "tokens = utils.REGEX_TOKENIZER.tokenize(raw_text)\n",
    "word_tokenized_text = ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize if there are differences between word clouds describing, respectively, raw text and tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_tokenize = utils.generate_word_cloud(text=word_tokenized_text, file_name='1_word_tokenize')\n",
    "word_cloud_tokenize.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see - and it is not really surprising - there is no significant changes after the tokenization part. Note that using the default tokenizer, we would end up with some words like 'nt' (which come from the decomposition of 'don't' in this case). Using Regexp tokenizer permit us to avoid such problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to remove all meaningless words which are present in the emails. Here, we use a set of English words defined as 'stopwords', assuming that all emails were written in such language.\n",
    "\n",
    "Also, some words, which are specific to the study, don't bring anything to our analysis and, in contrary, pollute it. Thus, we decide to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "filtered_text = ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of SPECIFIC_STOP_WORDS\n",
    "for word in utils.SPECIFIC_STOP_WORDS: \n",
    "    filtered_text = re.sub(r'\\b%s\\b' % word, '', filtered_text, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_stopwords = utils.generate_word_cloud(text=filtered_text, file_name='2_stopwords')\n",
    "word_cloud_stopwords.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even if we don't see big changes, the process removes for sure a lot of unwanted words, that may impact our further analysis if we don't remove them, and even if they don't appear as the most common ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Length before filtering: %d' % len(tokens))\n",
    "print('Length after filtering: %d' % len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize have some disadvantages for multiple reasons: \n",
    "* break up hyphenated sequence\n",
    "* be unsensible to lower case\n",
    "* usability/scalability.\n",
    "\n",
    "This is why, we should use differents method : Lemmatization and stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we tokenized and filtered the content, we may want to apply lemmatization and stemming to obtain the most common words. Indeed, in such text, words are declined in different combinaisons and we must count all of these combinaisons as unique ones for the kind of analysis we run here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: About the difference between stemming and lemmatization**\n",
    "\n",
    "The goal of both processes is to reduce inflectional forms or to find related forms of a word with a common base form ; however, the two techniques differ in the way they achieve to do it. Also, stemming was commonly implemented with reduction techniques, though this is not universal. Lemmatization, as for it, implies a possibly broader scope of functionality, which may include synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's firstly reduce inflectional (variant) forms to base form of the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#UTILS: use of WORDNET_LEMMATIZER\n",
    "wl_text = utils.WORDNET_LEMMATIZER.lemmatize(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_lemmatizer = utils.generate_word_cloud(text=wl_text, file_name='3_WordNetLemmatizer')\n",
    "word_cloud_lemmatizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use stemmer to reduce terms to their roots before indexing. Note that here we use both Porter stemmer, which is the most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of do_stemming_words, PORTER_STEMMER\n",
    "ps_stemming_text = utils.do_stemming_words(stemmer=utils.PORTER_STEMMER, words=wl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_porter_stemmer = utils.generate_word_cloud(text=ps_stemming_text, file_name='4_PorterStemmer')\n",
    "word_cloud_porter_stemmer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for curiosity, we can also use the Snowball stemmer to produce the same result. A good explanation was given about the differences between the main stemmer's algorithms <a href=\"http://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of do_stemming_words, SNOWBALL_STEMMER\n",
    "ss_stemming_text = utils.do_stemming_words(stemmer=utils.SNOWBALL_STEMMER, words=wl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of generate_word_cloud\n",
    "word_cloud_stemmer = utils.generate_word_cloud(text=ss_stemming_text, file_name='4_SnowballStemmer')\n",
    "word_cloud_stemmer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here, no big differences are observed between the two processing (respectively apply of Porter stemmer and Snowball stemmer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2 - Analysis on countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find all the mentions of world countries in the whole corpus, using the pycountry utility (HINT: remember that there will be different surface forms for the same country in the text, e.g., Switzerland, switzerland, CH, etc.) Perform sentiment analysis on every email message using the demo methods in the nltk.sentiment.util module. Aggregate the polarity information of all the emails by country, and plot a histogram (ordered and colored by polarity level) that summarizes the perception of the different countries. Repeat the aggregation + plotting steps using different demo methods from the sentiment analysis module -- can you find substantial differences?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we count all the occurrences of countries in Hillary Clinton's emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of count_countries_occurrences\n",
    "countries_occurrences = utils.count_countries_occurrences(ps_stemming_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_countries_occurrences = pd.DataFrame.from_dict(countries_occurrences, orient='index')\n",
    "df_countries_occurrences.columns = ['Occurrences']\n",
    "df_countries_occurrences.sort_values('Occurrences', ascending=False, inplace=True)\n",
    "df_countries_occurrences.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not surprising to find at the top of the list some countries like Israel, Libya, Haiti or even United States. Indeed, these countries are directly linked to the ex-Secretary of State!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the 30 most quoted countries to have a better observation of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of plot_most_quoted_countries\n",
    "utils.plot_most_quoted_countries(df_countries_occurrences, 30)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can remark the fact that the number of quotes of a country drastically fall. We need to pay attention to this detail in the next part of the study as an insufficient number of occurrences could change the sense of the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we want to attribute some kind of sentiment to each quoted country, according of what was discussed in the emails.\n",
    "\n",
    "*Note: For both analyzers, increment of positive and negative variables are done using returned score, and not in a static-way (e.g. increment of +1). Thus, we discriminate the case where the probability for a word (or a sentence) is high and another one where the probability for a word (respectively a sentence) is low, which is better. See details of the implementation in utils.py.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_emails_content = df_emails[['MetadataSubject', 'ExtractedBodyText']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use <a href=\"http://sentiwordnet.isti.cnr.it/\">SentiWordNet</a> in order to retrieve the sentimal score of each email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UTILS: use of retrieve_email_sentiment\n",
    "results = df_emails_content.apply(utils.retrieve_email_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_plot = pd.DataFrame(pd.value_counts(results['Type']))\n",
    "results_plot.plot(kind='bar', title='Number of occurrences for each type of emails (using SentiWordNet)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice from the previous graph, the number of positive emails is much more greater than the number of the neutral and negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use <a href=\"https://github.com/cjhutto/vaderSentiment\">Vader sentiment analysis</a> to see if we obtain different results.\n",
    "\n",
    "*Note: Some detailed examples on the performance of Vader sentiment analysis used in NLTK can be found <a href=\"http://www.nltk.org/howto/sentiment.html\">here</a>.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of retrieve_email_sentiment\n",
    "results_vader = df_emails_content.apply(utils.retrieve_email_sentiment, args=('Vader',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_vader_plot = pd.DataFrame(pd.value_counts(results_vader['Type']))\n",
    "result_vader_plot.plot(kind='bar', title='Number of occurrences for each type of emails (using Vader)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Vader sentiment analysis, we obtain a different distribution of the data. This difference can be explained by the fact that algorithm associated with SentiWordNet doesn't take into account the entire sentence and only associates sentiment to each word, individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have defined the sentiment of each email, let's define the sentiment associated to each country. Note that in order to avoid interferences, we only search for exact name of a country. Put in other words, we decide to ignore <a href=\"https://fr.wikipedia.org/wiki/ISO_3166-1_alpha-2\">ISO 3166-1 alpha-2</a> and <a href=\"https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3\">ISO 3166-1 alpha-3</a> country codes, as these codes can represent some words or abbreviations in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UTILS: use of get_countries_sentiment\n",
    "countries_sentiment = utils.get_countries_sentiment(results_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_sentiment = pd.DataFrame.from_dict(countries_sentiment, orient='index')\n",
    "df_countries_sentiment.columns = ['Sentiment']\n",
    "df_countries_sentiment.sort_values('Sentiment', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_sentiment.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_sentiment.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we display the 10 most-preferred and less-preferred countries, respectively, according to sentiment analysis. For some of them, there is explanations about the obtained score, even if it is not evident.\n",
    "\n",
    "For good scores, and as an example, <a href=\"https://en.wikipedia.org/wiki/Haiti%E2%80%93United_States_relations\">relations between Haiti and USA</a> can explain the obtained score. Also, even if USA was directly mobilized as part of Afghanistan's war, it does not mean that the sentiment toward this country is necessarily negative (quite the opposite!).\n",
    "\n",
    "Now, regarding negative scores, some recent events may explain them, as for Libya (see <a href=\"https://en.wikipedia.org/wiki/2012_Benghazi_attack\">2012 Benghazi attack</a> or <a href=\"https://en.wikipedia.org/wiki/Libyan_Civil_War_(2011)\">Libyan Civil War of 2011</a>). Some historical reasons also justify the bad sentiment toward some countries like Serbia (see <a href=\"https://en.wikipedia.org/wiki/Serbia%E2%80%93United_States_relations\">relations between Serbia and USA</a> and <a href=\"https://en.wikipedia.org/wiki/2008_Kosovo_declaration_of_independence\">declaration of independence of Kosovo</a>)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting sentiment and occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge occurrences and sentiment in order to have all the information in an unique DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_data = pd.merge(df_countries_occurrences, df_countries_sentiment, how='inner', left_index=True, right_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As sentiment seems to decrease exponentially, we decide to adjust the values using MinMaxScaler. More precisely, we use the log here because of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler((-1, 1))\n",
    "countries_data[['Sentiment']] = np.log(countries_data['Sentiment'] + abs(min(countries_data['Sentiment'])) + 10)\n",
    "countries_data['Sentiment'] = min_max_scaler.fit_transform(countries_data[['Sentiment']].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of plot_countries_by_occurrences_and_sentiment\n",
    "utils.plot_countries_by_occurrences_and_sentiment(countries_data, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Important note: Some results are more difficult to explain and are directly linked with the sentiment analyzers and their bias, but also with the fact that we ignored country codes during the process, while they are used in emails (it is about weighing up the pros and cons here). Also, and among other things, our assumptions and decisions explain the observed differences between different analysis on the same topic (see <a href=\"https://www.kaggle.com/operdeck/d/kaggle/hillary-clinton-emails/hillary-s-sentiment-about-countries\">another example on Kaggle</a>).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also display the countries according to their score. Here, we display the 20 most-preferred countries (left side) and the 20 less-preferred countries (right side). **Here, colors represent the number of occurrences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UTILS: use of plot_sentiment_by_country\n",
    "utils.plot_sentiment_by_country(countries_data, None, nb_country=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice that except for few countries (Lativia, Serbia or Lybia), bad feeling seems to be linked with few occurences. Hence, for these countries, we must pay attention and don't make any definitive conclusions!\n",
    "\n",
    "Note that, in contrary, it seems to be a correlation between good sentiment and number of occurrences for a given country, except for Libya..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3 - Retrieve of the main topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the models.ldamodel module from the gensim library, run topic modeling over the corpus. Explore different numbers of topics (varying from 5 to 50), and settle for the parameter which returns topics that you consider to be meaningful at first sight.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to draw attention on the fact that, in this part, we describe the topics manually. Put another way, the conclusions that are made here are subjective and may differ with the reader's ones.\n",
    "\n",
    "We can be tempted to use automatic recognition of topics, but according to a research (<a href=\"http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2009_0125.pdf\">see paper</a>), the better way to do this is to use human cognition when it comes to compare two LDA models (and this is even more true when two LDA models seem to by very close, as it is more difficult to conclude about the accuracy of the model itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First method: using raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we use the raw text as source for creating our corpus and then the LDA model. No processing or strategies are applied at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first, we create our corpus using all content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_raw, id2word_raw = utils.create_corpus(df_emails['ExtractedBodyText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create our LDA model with different number of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create LDA models with different number of topics and see if we observe significant differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_5_topics_raw = utils.create_lda_model(corpus_raw, id2word_raw, 5)\n",
    "lda_5_topics_raw.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, and as expected, the topics are not so similar. These topics are about the 44th President of the United States (Barack Obama) and some business related with Secretary's job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_10_topics_raw = utils.create_lda_model(corpus_raw, id2word_raw, 10)\n",
    "lda_10_topics_raw.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10 topics, we start to get different subjects, which are not linked all together. For example, we have some topics regarding Israel and public relations between Israel and the United States, but also a topic regarding actions (say, make, think, see, and so on). Finally, we continue to find some topics linked with Secretary's job.\n",
    "\n",
    "Notice that some topics are not clear (mixing of garbage with real subjects for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_25_topics_raw = utils.create_lda_model(corpus_raw, id2word_raw, 25)\n",
    "lda_25_topics_raw.print_topics(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the number of topics to 25 permits to have a better understanding of what is going on and what was discussed in the mails. However, garbage is still present, but it was expected.\n",
    "\n",
    "Among other topics, we have: China, domestic policy, Afghanistan's case, Israeli–Palestinian conflict, surveys (maybe because of the United States presidential election of 2016?), common business of Secretary, U.S. President and U.S. presidency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_50_topics_raw = utils.create_lda_model(corpus_raw, id2word_raw, 50)\n",
    "lda_50_topics_raw.print_topics(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we still find the same topics than the previous ones, while others were added, like Iraq war or nuclear program of Iran.\n",
    "\n",
    "Notice that when the number of topics is higher, garbage is, in some way, reduced (which is quite good as these entries don't bring anything to the study). Also, each LDA model seems to be quite identical at first sight, even if we play with the parameters.\n",
    "\n",
    "On the whole, we observe what we expected: with few number of topics, there are no real links, while with a higher number, the links become more explicit (even if there are different topics at the end)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second method: using cleaned text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we apply all the processing pipeline (cleaning of text) before creating our model, in order to see if there is any difference at the end.\n",
    "\n",
    "*Note: We only display 5 and 50 topics here, for the sake of concision.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = utils.SENTENCES_DETECTOR.tokenize(ps_stemming_text.strip())\n",
    "corpus_clean, id2word_clean = utils.create_corpus(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_5_topics_clean = utils.create_lda_model(corpus_clean, id2word_clean, 5)\n",
    "lda_5_topics_clean.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with 5 topics, we don't observe any significant changes... The main difference is that here we have less garbage (a lot of unwanted characters or \"word-like\" strings were deleted). This is quite normal, as we use a clean and processed version of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_50_topics_clean = utils.create_lda_model(corpus_clean, id2word_clean, 50)\n",
    "lda_50_topics_clean.print_topics(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remark that with 50 topics, we don't have any differences (i.e. topics are similar, as expected). We suspect the fact that some stages of the processing part directly affect the results (stemming, lemmazitation)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third method: aggregate by conversations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we decide to aggregate the different emails in conversations. This approach is indeed better understandable as the goal here is to find topics in all sent emails.\n",
    "\n",
    "Please note that, again, for the sake of concision, we only display 5 and 50 topics.\n",
    "\n",
    "Also, we decide to use raw text. However, similar analysis can be done with cleaned text and, with this in mind, we provide to the reader a function which applies all the cleaning process, as done in Q1. For more information, *see process_email_content* in *utils.py* and comments in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'ExtractedBodyText'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INDICATION: Uncomment the content of this cell in order to use cleaned text!\n",
    "# text = 'CleanedBodyText'\n",
    "# df_emails_content[text] = df_emails_content.apply(utils.process_email_content, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a lot of emails don't have any subject, we decide to separate them from the other ones, as we definitively can't assume that they constitute a single conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "untitled_emails = df_emails_content[df_emails_content['MetadataSubject'] == '']\n",
    "titled_emails = df_emails_content[df_emails_content['MetadataSubject'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all emails with a title, we make groups regarding the subject (i.e. we assume that two mails with the same subject constitute a single conversation) and we merge contents.\n",
    "\n",
    "*Note: Please find a discussion about the limitations of this method at the end of current part.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated_content = titled_emails.groupby('MetadataSubject').apply(lambda x: ' '.join(x[text])).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then store the different contents in an array which will be used to create the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_text_array = []\n",
    "all_text_array += utils.get_text_without_stopwords(aggregated_content)\n",
    "all_text_array += utils.get_text_without_stopwords(untitled_emails[text].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus_conversations, id2word_conversations = utils.create_corpus(all_text_array, processed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_5_topics_conversations = utils.create_lda_model(corpus_conversations, id2word_conversations, 5)\n",
    "lda_5_topics_conversations.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we find some topics that were displayed in previous parts. Notice that for some topics, here, it is more difficult to define them... Also, we still have some garbage as we use, by default, raw text (see comment at beginning of third part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_50_topics_conversations = utils.create_lda_model(corpus_conversations, id2word_conversations, 50)\n",
    "lda_50_topics_conversations.print_topics(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are some topics that we previously found using different methods. However, using such method permits us to find also new topics (focused, among other things, on diplomacy, which is not surprising at all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion of the different methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we presented three possible methods to generate topics from emails content. An interesting fact is that, contrary to what has been thought, the first method is sufficient to find interesting topics and other ones don't significantly improve the results in our case...\n",
    "\n",
    "In fact, these methods have all some pros and cons.\n",
    "\n",
    "<table style=\"border-collapse:collapse;border-spacing:0;border-color:#aaa\"><tr><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#fff;background-color:#fd6864;vertical-align:top\"></th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#fff;background-color:#fd6864;text-align:center;vertical-align:top\">Pros</th><th style=\"font-family:Arial, sans-serif;font-size:14px;font-weight:bold;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#fff;background-color:#fd6864;text-align:center;vertical-align:top\">Cons</th></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center;vertical-align:top\">First method<br>(raw text)</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;vertical-align:top\">- Use of the words as they are<br>- No specific processing needed</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;vertical-align:top\">- Lot of garbage<br>- Possible bias (garbage replacing interesting keywords)</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center;vertical-align:top\">Second method<br>(cleaned text)</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;vertical-align:top\">- Higher identification of keywords</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;vertical-align:top\">- Loss of signification of some specific keywords<br>- Cost of the processing</td></tr><tr><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;font-weight:bold;text-align:center;vertical-align:top\">Third method<br>(aggregate)</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;vertical-align:top\">- Best way to find topics<br>- Can be used with first and second methods</td><td style=\"font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#aaa;color:#333;background-color:#fff;vertical-align:top\">- Cost++ of the processing<br>- Complexity of the processing</td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note regarding third method**\n",
    "\n",
    "Here, we use a naive approach which introduces bias in the results. Indeed, we group the different emails according to their titles (i.e. we assume that two emails that have the same subject correspond to the same conversation). It goes without saying that this is a strong assumption which is not quite accurate. A better approach would be to check also the date of receive/send to ensure that the emails effectively correspond to the same conversation, but also the content (see next paragraph).\n",
    "\n",
    "Also, there is another huge problem here: when grouping emails by title, we simply aggregate all the contents. However, all responses or forwards contain the history of the discussion by default; A simple aggregation means that, as well as having garbage (headers of previous emails), we obtain replications of the same text and this can impact, at the end of the process, the generation of topics. We may only consider the email with most content (suppossing that all the conversation is in it) and throw all headers, but this would require a lot of processing and even with such approach, some problems would be still present (for example, what if before replying, someone delete the history of the email?).\n",
    "\n",
    "As the final objective here is not to provide a precise, perfect, pipeline but only a global vision of what is going on, a lot of simplifications were made. For all that, we must keep in mind all these issues!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "<b>Build the communication graph (unweighted and undirected) among the different email senders and recipients using the NetworkX library. Find communities in this graph with community.best_partition(G) method from the community detection module. Print the most frequent 20 words used by the email authors of each community. Do these word lists look similar to what you've produced at step 3 with LDA? Can you identify clear discussion topics for each community? Discuss briefly the obtained results.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we build the full graph containing all the communications. For each email, we add a link between the sender and the receiver (we optionally add the weight of the link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = defaultdict(lambda : defaultdict(int))\n",
    "nodes = set()\n",
    "for email in df_emails.itertuples():\n",
    "    senderId = email.SenderPersonId\n",
    "    if senderId:\n",
    "        nodes |= {int(senderId)}\n",
    "        receivers = df_email_receivers.loc[df_email_receivers['EmailId'] == email.Index]\n",
    "        for receiver in receivers.itertuples():\n",
    "            nodes |= {int(receiver.PersonId)}\n",
    "            links[int(senderId)][int(receiver.PersonId)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of nodes: ' + str(len(nodes)))\n",
    "print('Number of links: ' + str(sum([len(receivers) for sender, receivers in links.items()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build a graphic representation, we need a simple list containing all links. To do so, we flatten the links' dictionnary that was built before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_links = []\n",
    "for sender, receivers in links.items():\n",
    "    for receiver, weight in receivers.items():\n",
    "        list_links.append((sender, receiver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of links: ' + str(len(list_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the first graph with all links and all nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for node in nodes:\n",
    "    name = df_persons.loc[1]['Name']\n",
    "    G.add_node(node, id=node, name=name)\n",
    "G.add_edges_from(list_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nx.draw_circular(G, node_size=30, node_color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we can see, there are a lot of nodes and links and thus graph is not relevant at all. Let's try with the `community` tools to find partitions and at the same time fetch emails for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base on example provided here: http://perso.crans.org/aynaud/communities/index.html\n",
    "partition = community.best_partition(G)\n",
    "\n",
    "size = float(len(set(partition.values())))\n",
    "pos = nx.spring_layout(G)\n",
    "count = 0\n",
    "communities = {}\n",
    "for communityId in set(partition.values()):\n",
    "    \n",
    "    # Build graph\n",
    "    count = count + 1\n",
    "    list_nodes = [partition_nodes for partition_nodes in partition.keys() if partition[partition_nodes] == communityId]\n",
    "    nx.draw_networkx_nodes(G, pos, list_nodes, node_size=20, node_color=str(count / size))\n",
    "    \n",
    "    # Build communities with emails\n",
    "    content = ''\n",
    "    for personId in list_nodes:\n",
    "        content += ' '.join([email.MetadataSubject + ' ' + email.ExtractedBodyText for email in df_emails.loc[df_emails['SenderPersonId'] == personId].itertuples()]).lower()\n",
    "    communities[communityId] = {\n",
    "        'document': content,\n",
    "        'nodes': list_nodes,\n",
    "        'counter': Counter(content.split())\n",
    "    }\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of communities: ' + str(len(set(partition.values()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that there is only one real community (with Clinton at the middle for sure) and few others, with limited links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can retrieve the most used words in each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key, com in communities.items():\n",
    "    print('Words: ', end='')\n",
    "    for word, number in com['counter'].most_common(20):\n",
    "        print('\"'+ word + '\" (' + str(number) + ')', end=', ')\n",
    "    print('\\n-----------------------')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
