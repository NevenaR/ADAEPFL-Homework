{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 05 - Taming text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "from PIL import Image\n",
    "from os import path\n",
    "from os.path import exists\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn import preprocessing\n",
    "from gensim import models, corpora\n",
    "\n",
    "# data processing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk import\n",
    "from nltk.tag import PerceptronTagger\n",
    "from nltk.corpus import stopwords, subjectivity\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.data\n",
    "\n",
    "# utils\n",
    "import pycountry\n",
    "import random\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from os.path import exists\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Background used in this Notebook was made by <a href=\"http://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> from <a href=\"http://www.flaticon.com\" title=\"Flaticon\">www.flaticon.com</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set of english token\n",
    "SENTENCES_DETECTOR = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# tools to detect sentiments in words\n",
    "SID = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Regex Tokenizer \n",
    "REGEX_TOKENIZER = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def generate_raw_text(data):\n",
    "    '''\n",
    "    Generate a String from an array of string\n",
    "    attributes:\n",
    "        - data : array of String to transform\n",
    "    \n",
    "    return value String\n",
    "    '''\n",
    "    text = ''\n",
    "    for d in data:\n",
    "        text += str(d) + ' '\n",
    "    return text\n",
    "\n",
    "\n",
    "def do_stemming_words(stemmer, words):\n",
    "    '''\n",
    "    Generate a String applying a stemmer on each words of an array\n",
    "    attributes:\n",
    "        - stemmer : apply stemmer to each word\n",
    "        - words   : array of String to transform\n",
    "    \n",
    "    return value String\n",
    "    '''\n",
    "    text = ''\n",
    "    for w in words:\n",
    "        text += stemmer.stem(w)\n",
    "    return text\n",
    "\n",
    "\n",
    "def grey_color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return \"hsl(0, 0%%, %d%%)\" % random.randint(40, 60)\n",
    "\n",
    "\n",
    "def generate_word_cloud(text, img_name='envelope.png', max_words=1000, width=900, height=900, dpi=400, file_name=None):\n",
    "    '''\n",
    "    Generate and display a word cloud from a text removing tokens (STOPWORD)\n",
    "    attributes:\n",
    "        - text      : text to study\n",
    "        - img_name  : name of the picture used for the word cloud.\n",
    "        - max_words : limit max of words\n",
    "        - width     : width of the picture\n",
    "        - height    : height of the picture\n",
    "        - dpi       : quality of the picture\n",
    "        - file_name : name of the picture in case you want to save the picture (the picture is saved in ./images/) \n",
    "    '''\n",
    "    stopwords = set(STOPWORDS)\n",
    "    mask = np.array(Image.open(img_name))\n",
    "    wc = WordCloud(background_color=\"white\", mask=mask, max_words=max_words, stopwords=stopwords).generate(text)\n",
    "    plt.figure(figsize=(9, 9), dpi=dpi)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(wc.recolor(color_func=grey_color_func, random_state=3))\n",
    "    if file_name:\n",
    "        path = './images/' + file_name + '.png'\n",
    "        if not exists(path):\n",
    "            plt.savefig(path, dpi=dpi)\n",
    "    return plt\n",
    "\n",
    "\n",
    "def get_country_name(word):\n",
    "    '''\n",
    "    Get the contry if exist in dictionnary\n",
    "    attributes:\n",
    "        - word : text to study\n",
    "        \n",
    "    return value None or the name of the contry if exist in dictionnary\n",
    "    '''\n",
    "    lower_word = str.lower(word)\n",
    "    for c in pycountry.countries:\n",
    "        if (word == c.alpha_2) or (word == c.alpha_3) or (lower_word == str.lower(c.name)) or (hasattr(c, 'official_name') and (lower_word == str.lower(c.official_name))):\n",
    "            return c.name\n",
    "    return None\n",
    "\n",
    "\n",
    "def check_if_country_in_text(country, text):\n",
    "    '''\n",
    "    return the number of time, the text contain the contry name\n",
    "    \n",
    "    attributes:\n",
    "        - country :\n",
    "        - text   : text to study\n",
    "        \n",
    "    return value : boolean\n",
    "    '''\n",
    "    return ((str.lower(country.name) in text) or (hasattr(country, 'official_name') and (str.lower(country.official_name) in text)))\n",
    "\n",
    "def count_country_occurrences(country, text):\n",
    "    '''\n",
    "    return the number of time, the text contain the contry name\n",
    "    attributes:\n",
    "        - country : name of the contry\n",
    "        - text    : text to study\n",
    "        \n",
    "    return value : integer\n",
    "    ''' \n",
    "    nb_occurrences = 0\n",
    "    #nb_occurrences += text.count(country.alpha_2)\n",
    "    #nb_occurrences += text.count(country.alpha_3)\n",
    "    nb_occurrences += text.count(str.lower(country.name))\n",
    "    if hasattr(country, 'official_name'):\n",
    "        nb_occurrences += text.count(str.lower(country.official_name))\n",
    "    return nb_occurrences\n",
    "\n",
    "\n",
    "def count_countries_occurrences(text):\n",
    "    '''\n",
    "    Count for each country the number of time the country appear in the text\n",
    "    attributes:\n",
    "        - text : text to study\n",
    "        \n",
    "    return value : array containing for each contry the number of apparition.\n",
    "    '''\n",
    "    lower_text = str.lower(text)\n",
    "    countries = Counter()\n",
    "    for country in pycountry.countries:\n",
    "        nb_occurrences = count_country_occurrences(country, lower_text)\n",
    "        countries[country.name] = nb_occurrences\n",
    "    return countries\n",
    "\n",
    "    \n",
    "def get_wordnet_tag_type(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_sentiwordnet_scores(tokens):   \n",
    "    '''\n",
    "    Associate a feeling POSITIVE / NEGATIVE from words present in token\n",
    "    attributes:\n",
    "        - tokens : text to study\n",
    "        \n",
    "    return value : map containing the number of positive/negative values\n",
    "    '''\n",
    "    types = Counter({'Positive': 0, 'Negative': 0})\n",
    "    for word, pos_tag in nltk.pos_tag(tokens):\n",
    "        tag = get_wordnet_tag_type(pos_tag)\n",
    "        synset_list = list(swn.senti_synsets(word, pos=tag))\n",
    "        if synset_list:\n",
    "            types['Positive'] += synset_list[0].pos_score()\n",
    "            types['Negative'] += synset_list[0].neg_score()\n",
    "        else:\n",
    "            continue\n",
    "    return types\n",
    "\n",
    "\n",
    "def get_vader_scores(email_content):\n",
    "    '''\n",
    "    Associate a feeling POSITIVE / NEGATIVE from words present in email_content\n",
    "    attributes:\n",
    "        - email_content : text to study\n",
    "        \n",
    "    return value : map containing the number of positive/negative values\n",
    "    '''\n",
    "    types = Counter({'Positive': 0, 'Negative': 0})\n",
    "    tokens = SENTENCES_DETECTOR.tokenize(email_content.strip())\n",
    "    for sentence in tokens:\n",
    "        scores = SID.polarity_scores(sentence)\n",
    "        types['Positive'] += scores['pos']\n",
    "        types['Negative'] += scores['neg']\n",
    "    return types\n",
    "\n",
    "''' \n",
    "def retrieve_email_sentiment(email, analyzer='sentiwordnet'):\n",
    "  \n",
    "    email_content = str(email['ExtractedSubject']) + ' ' + str(email['ExtractedBodyText'])\n",
    "    if(analyzer == 'Vader'):\n",
    "        types = get_vader_scores(email_content)\n",
    "    else:\n",
    "        tokens = REGEX_TOKENIZER.tokenize(email_content)\n",
    "        types = get_sentiwordnet_scores(tokens)\n",
    "    \n",
    "    if types['Positive'] > abs(types['Negative']):\n",
    "        email['Type'] = 'Positive'\n",
    "    elif abs(types['Negative']) > types['Positive']:\n",
    "        email['Type'] = 'Negative'\n",
    "    else:\n",
    "        email['Type'] = 'Neutral'\n",
    "        \n",
    "    return email\n",
    "\n",
    "'''\n",
    "\n",
    "def retrieve_email_sentiment(email, analyzer='sentiwordnet'):\n",
    "  \n",
    "    email_content = str(email['ExtractedSubject']) + ' ' + str(email['ExtractedBodyText'])\n",
    "    typesVader = get_vader_scores(email_content)\n",
    "    tokens = REGEX_TOKENIZER.tokenize(email_content)\n",
    "    typesSentiWord = get_sentiwordnet_scores(tokens)\n",
    "    \n",
    "    types = {\n",
    "             'Positive': (0,6 * typesVader['Positive'] + 0,4 * typesSentiWord['Positive']),\n",
    "             'Negative': (0,6 * typesVader['Negative'] + 0,4 * typesSentiWord['Negative'])\n",
    "            }\n",
    "\n",
    "    if types['Positive'] > types['Negative']:\n",
    "        email['Type'] = 'Positive'\n",
    "    elif types['Negative'] > types['Positive']:\n",
    "        email['Type'] = 'Negative'\n",
    "    else:\n",
    "        email['Type'] = 'Neutral'\n",
    "        \n",
    "    return email\n",
    "\n",
    "\n",
    "def get_countries_sentiment(emails):\n",
    "    countries_sentiment = Counter()\n",
    "    for index, email in emails.iterrows():\n",
    "        email_content = str(email['ExtractedSubject']) + ' ' + str(email['ExtractedBodyText'])\n",
    "        lower_email_content = str.lower(email_content)\n",
    "        for country in pycountry.countries:\n",
    "            is_in_text = check_if_country_in_text(country, lower_email_content)\n",
    "            if(is_in_text == True):\n",
    "                if email['Type'] == 'Positive':\n",
    "                    countries_sentiment[country.name] += 1\n",
    "                elif email['Type'] == 'Negative':\n",
    "                    countries_sentiment[country.name] -= 1\n",
    "                else:\n",
    "                    countries_sentiment[country.name] += 0\n",
    "    return countries_sentiment\n",
    "\n",
    "\n",
    "def plot_sentiment_by_contry(data_mails,opt,nb_contry = 20):\n",
    "    '''\n",
    "    Plot the number of country in axis, the number of occurence in ordinate\n",
    "    and use 4 colors for the sentiments associate with the country. \n",
    "    attributes: \n",
    "        - opt : allow to select the option of the graph: \n",
    "                - 'only_good' = keep only good feeling about the contry\n",
    "                - 'only_bad' = keep only bad feeling about the contry\n",
    "                -            = concat the good and bad feeling in the same plot.\n",
    "        - data_mails : dataframe containing the data of the mails\n",
    "        - nb_contry : Selection on the most representative country\n",
    "    '''\n",
    "    # Selecting interesting data for the plot.\n",
    "    if opt == 'only_good':\n",
    "        data_plot = data_mails.nlargest(nb_contry, 'Sentiment')\n",
    "        title = 'Hilary\\'s opinion on the ' + str(nb_contry) + ' best feelings about contry'\n",
    "    elif opt == 'only_bad':\n",
    "        data_plot = data_mails.nsmallest(nb_contry, 'Sentiment')\n",
    "        title = 'Hilary\\'s opinion on the ' + str(nb_contry) + ' worst feelings about country'\n",
    "    else:\n",
    "        most_liked = data_mails.nlargest(nb_contry, 'Sentiment')\n",
    "        worst_liked = data_mails.nsmallest(nb_contry, 'Sentiment')\n",
    "        data_plot = pd.concat([most_liked,worst_liked])\n",
    "        title = 'Hilary\\'s opinion on the ' + str(nb_contry) + ' worst/best feelings about country'\n",
    "        \n",
    "    data_plot.sort_values(by='Sentiment', ascending=False, inplace=True)\n",
    "    \n",
    "    max_occurence = max(countries_data.Occurrences)\n",
    "    divide_max_occurence = max_occurence/4\n",
    "\n",
    "    # Define the gradation of color\n",
    "    colors = ['green' if s > max_occurence-divide_max_occurence \n",
    "         else 'palegreen' if (s < max_occurence-divide_max_occurence and s > max_occurence-2*divide_max_occurence) \n",
    "         else 'sandybrown' if (s < max_occurence-2*divide_max_occurence and s > max_occurence-3*divide_max_occurence) \n",
    "         else 'red' for s in data_plot['Occurrences']]\n",
    "\n",
    "    map_color_legend = ['Lot of occurence', 'some occurence', 'Few occurence', 'Very few occurence']\n",
    "    # build the plot\n",
    "    sentiment_data_plot = sns.barplot(x=data_plot.index, y='Sentiment', data=data_plot, palette=colors)\n",
    "    # display a line to separate the graph.\n",
    "    define_plot_legend(sentiment_data_plot,map_color_legend,title=title)\n",
    "    \n",
    "    if opt == None and nb_contry == 20:\n",
    "        sentiment_data_plot.axvline(nb_contry - 0.5)\n",
    "    sns.plt.show()\n",
    "    \n",
    "def define_plot_legend (plot,map_color_legend,label_y='Sentiment',title='Hilary\\'s opinion on the 20 most-quoted countries'):\n",
    "    '''\n",
    "    Define the legend,title and label of a plot\n",
    "    attribute:\n",
    "        - plot : seaborn plot to modify\n",
    "        - map_color_legend : label for différent color in order green,palegreen\n",
    "          sandybrown and red.\n",
    "        - label :ylabel\n",
    "        - title : plot title\n",
    "    '''\n",
    "    for label in plot.get_xticklabels():\n",
    "        label.set_rotation(90)\n",
    "        \n",
    "    plot.set(ylabel=label_y)\n",
    "    plot.set_title(title)\n",
    "    \n",
    "    # Set color\n",
    "    green_legend = mpatches.Patch(color='green', linewidth=0)\n",
    "    palegreen_legend = mpatches.Patch(color='palegreen', linewidth=0)\n",
    "    sandybrown_legend = mpatches.Patch(color='sandybrown', linewidth=0)\n",
    "    red_legend = mpatches.Patch(color='red', linewidth=0)\n",
    "    \n",
    "    # Set legend\n",
    "    plt.legend((green_legend, palegreen_legend, sandybrown_legend, red_legend), map_color_legend)\n",
    "    \n",
    "def plot_most_occurence_contry(data_mails,nb_contry = 20):\n",
    "    '''\n",
    "    Plot the number of country in axis, the number of occurence in ordinate\n",
    "    and use 4 colors for the sentiments associate with the country. \n",
    "    attributes: \n",
    "        - data_mails : dataframe containing the data of the mails\n",
    "        - nb_contry : Selection on the most representative country\n",
    "    '''\n",
    "    # select the data for plotting\n",
    "    twenty_most_quoted_countries = data_mails.nlargest(nb_contry, 'Occurrences')\n",
    "\n",
    "    # Define the gradation of color in order to display three variable\n",
    "    colors = ['green' if s > 0.5 \n",
    "         else 'palegreen' if (s < 0.5 and s > 0) \n",
    "         else 'sandybrown' if (s < 0 and s > -0.5) \n",
    "         else 'red' for s in twenty_most_quoted_countries['Sentiment']]\n",
    "\n",
    "    countries_data_plot = sns.barplot(x=twenty_most_quoted_countries.index, y='Occurrences', data=twenty_most_quoted_countries, palette=colors)\n",
    "    map_color_legend = ['Very good opinion', 'Good opinion', 'Bad opinion', 'Very bad opinion']\n",
    "    define_plot_legend(countries_data_plot,map_color_legend,'Occurrences',title='Hilary\\'s opinion on the ' + str(nb_contry) + ' most-quoted countries')\n",
    "    sns.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all data as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aliases = pd.read_csv('hillary-clinton-emails/Aliases.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aliases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_receivers = pd.read_csv('hillary-clinton-emails/EmailReceivers.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_receivers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_emails = pd.read_csv('hillary-clinton-emails/Emails.csv', index_col=0)\n",
    "df_emails.fillna('', inplace=True)\n",
    "df_emails.replace({'\\n': ' '}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_persons = pd.read_csv('hillary-clinton-emails/Persons.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_persons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the text in the body of each mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emails_content = df_emails['ExtractedSubject'] + ' ' + df_emails['ExtractedBodyText']\n",
    "raw_text = generate_raw_text(data=emails_content.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We generate the word cloud without any further modifications. <br/>\n",
    "> The word cloud below contains important words use in the hilary's mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have noticed that certain amount of words are specific to mail. \n",
    "# these words don't bring anything important in the word cloud, we have decided to remove them.\n",
    "specific_mail_words = ['Fw','Re','pm']\n",
    "\n",
    "for word in specific_mail_words: \n",
    "    raw_text = raw_text.replace(word,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_cloud = generate_word_cloud(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply different processes for cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist we try to tokenize the hilary's emails <br/>\n",
    "\n",
    "* A token is an instance of a sequence of characters\n",
    "* Each such token is now a candidate for an index entry, after further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = REGEX_TOKENIZER.tokenize(raw_text)\n",
    "word_tokenized_text = generate_raw_text(data=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's visualize the difference with the same wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_tokenize = generate_word_cloud(text=word_tokenized_text, file_name='1_word_tokenize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_tokenize.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our goal is to remove all meaningless words present in emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stopwords contains \"meaningless\" words\n",
    "filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "filtered_text = generate_raw_text(data=filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_cloud_stopwords = generate_word_cloud(text=filtered_text, file_name='2_stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_stopwords.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize have some disadvantages for multiple reasons: \n",
    "* break up hyphenated sequence\n",
    "* be unsensible to lower case\n",
    "* usability/scalability.\n",
    "\n",
    "This is why, we should use differents method : Lemmatization and stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lemmatize hilari's emails, \n",
    ">Reduce inflectional/variant forms to base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wl = WordNetLemmatizer()\n",
    "wl_text = wl.lemmatize(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_cloud_lemmatizer = generate_word_cloud(text=wl_text, file_name='3_WordNetLemmatizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_lemmatizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's stemmerize hilari's emails :\n",
    ">Reduce terms to their “roots” before indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "ps_stemming_text = do_stemming_words(stemmer=ps, words=wl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_cloud_porter_stemmer = generate_word_cloud(text=ps_stemming_text, file_name='4_PorterStemmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_porter_stemmer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Difference between Stemming and lemmatization: </b> <br/>\n",
    "<p>The goal of both processes is to reduce inflectional forms or to find related forms of a word to a common base form, but the two techniques differ : </p>\n",
    "\n",
    ">Both stemming and lemmatization allow queries to match different forms of words.  Stemming was commonly implemented with Reduction techniques, though this is not universal.  Lemmatization implies a possibly broader scope of functionality, which may include synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = SnowballStemmer(\"english\")\n",
    "ss_stemming_text = do_stemming_words(stemmer=ss, words=wl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_cloud_stemmer = generate_word_cloud(text=ss_stemming_text, file_name='4_SnowballStemmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_stemmer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_occurrences = count_countries_occurrences(ps_stemming_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_occurrences = pd.DataFrame.from_dict(countries_occurrences, orient='index')\n",
    "df_countries_occurrences.columns = ['Occurrences']\n",
    "df_countries_occurrences.sort_values('Occurrences', ascending=False, inplace=True)\n",
    "df_countries_occurrences.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_quoted_countries = df_countries_occurrences.head(15)\n",
    "countries_plot = sns.barplot(x=most_quoted_countries.index, y='Occurrences', data=most_quoted_countries, color='lightblue')\n",
    "for label in countries_plot.get_xticklabels():\n",
    "    label.set_rotation(90)\n",
    "countries_plot.set(ylabel='Occurrences')\n",
    "countries_plot.set_title('Number of occurrences of 15 most-quoted countries')\n",
    "sns.plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_emails_content = df_emails[['ExtractedSubject', 'ExtractedBodyText']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = df_emails_content.apply(retrieve_email_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_vader = df_emails_content.apply(retrieve_email_sentiment, args=('Vader',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_sentiment = get_countries_sentiment(results_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_sentiment = pd.DataFrame.from_dict(countries_sentiment, orient='index')\n",
    "df_countries_sentiment.columns = ['Sentiment']\n",
    "df_countries_sentiment.sort_values('Sentiment', ascending=False, inplace=True)\n",
    "df_countries_sentiment.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_data = pd.merge(df_countries_occurrences, df_countries_sentiment, how='inner', left_index=True, right_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler((-1, 1))\n",
    "countries_data['Sentiment'] = min_max_scaler.fit_transform(countries_data[['Sentiment']].astype(float).as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_most_occurence_contry(countries_data,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_sentiment_by_contry(countries_data,'only_good',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_sentiment_by_contry(countries_data,'only_bad',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The separation between good and bad feelings about the data can be drawn, but\n",
    "plot_sentiment_by_contry(countries_data,None,nb_contry=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "> <span style='color:red'>TODO:</span>\n",
    "<br/>\n",
    "> Verify validity of results\n",
    "<br/>\n",
    "> Use demo from nltk.sentiment.util</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = SENTENCES_DETECTOR.tokenize(ps_stemming_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "# http://christop.club/2014/05/06/using-gensim-for-lda/\n",
    "# http://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python\n",
    "all_text_array = [[word for word in sentence.lower().split()] for sentence in sentences]\n",
    "dictionary = corpora.Dictionary(all_text_array)\n",
    "\n",
    "id2word = {}\n",
    "for word in dictionary.token2id:    \n",
    "    id2word[dictionary.token2id[word]] = word\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in all_text_array]\n",
    "lda = models.LdaModel(corpus, id2word=id2word, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
