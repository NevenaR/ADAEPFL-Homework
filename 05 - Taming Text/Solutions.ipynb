{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 05 - Taming text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data processing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from gensim import models, corpora\n",
    "\n",
    "# nltk import\n",
    "\n",
    "from nltk.corpus import stopwords, subjectivity\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.data\n",
    "\n",
    "# utils\n",
    "import pycountry\n",
    "import random\n",
    "from PIL import Image\n",
    "from os import path\n",
    "from os.path import exists\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "%matplotlib inline  \n",
    "\n",
    "\n",
    "# our code (mark it at autoreload at every cell execution - useful in developement mode)\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Background used in this Notebook was made by <a href=\"http://www.flaticon.com/authors/freepik\" title=\"Freepik\">Freepik</a> from <a href=\"http://www.flaticon.com\" title=\"Flaticon\">www.flaticon.com</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all data as DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aliases = pd.read_csv('hillary-clinton-emails/Aliases.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_aliases.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_receivers = pd.read_csv('hillary-clinton-emails/EmailReceivers.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_receivers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_emails = pd.read_csv('hillary-clinton-emails/Emails.csv', index_col=0)\n",
    "df_emails.fillna('', inplace=True)\n",
    "df_emails.replace({'\\n': ' '}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_persons = pd.read_csv('hillary-clinton-emails/Persons.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_persons.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling raw text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to extract the text in the body of each mail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emails_content = df_emails['ExtractedSubject'] + ' ' + df_emails['ExtractedBodyText']\n",
    "raw_text = utils.generate_raw_text(data=emails_content.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We generate the word cloud without any further modifications. <br/>\n",
    "> The word cloud below contains important words use in the hilary's mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have noticed that certain amount of words are specific to mail. \n",
    "# these words don't bring anything important in the word cloud, we have decided to remove them.\n",
    "specific_mail_words = ['Fw','Re','pm']\n",
    "\n",
    "for word in specific_mail_words: \n",
    "    raw_text = raw_text.replace(word,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_cloud = utils.generate_word_cloud(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply different processes for cleaning the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frist we try to tokenize the hilary's emails <br/>\n",
    "\n",
    "* A token is an instance of a sequence of characters\n",
    "* Each such token is now a candidate for an index entry, after further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = utils.REGEX_TOKENIZER.tokenize(raw_text)\n",
    "word_tokenized_text = utils.generate_raw_text(data=tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's visualize the difference with the same wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_tokenize = utils.generate_word_cloud(text=word_tokenized_text, file_name='1_word_tokenize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_tokenize.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our goal is to remove all meaningless words present in emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stopwords contains \"meaningless\" words\n",
    "filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "filtered_text = utils.generate_raw_text(data=filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_cloud_stopwords = utils.generate_word_cloud(text=filtered_text, file_name='2_stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_stopwords.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize have some disadvantages for multiple reasons: \n",
    "* break up hyphenated sequence\n",
    "* be unsensible to lower case\n",
    "* usability/scalability.\n",
    "\n",
    "This is why, we should use differents method : Lemmatization and stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lemmatize hilari's emails, \n",
    ">Reduce inflectional/variant forms to base form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wl = WordNetLemmatizer()\n",
    "wl_text = wl.lemmatize(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_lemmatizer = utils.generate_word_cloud(text=wl_text, file_name='3_WordNetLemmatizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_lemmatizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's stemmerize hilari's emails :\n",
    ">Reduce terms to their “roots” before indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "ps_stemming_text = utils.do_stemming_words(stemmer=ps, words=wl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_porter_stemmer = utils.generate_word_cloud(text=ps_stemming_text, file_name='4_PorterStemmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_porter_stemmer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Difference between Stemming and lemmatization: </b> <br/>\n",
    "<p>The goal of both processes is to reduce inflectional forms or to find related forms of a word to a common base form, but the two techniques differ : </p>\n",
    "\n",
    ">Both stemming and lemmatization allow queries to match different forms of words.  Stemming was commonly implemented with Reduction techniques, though this is not universal.  Lemmatization implies a possibly broader scope of functionality, which may include synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = SnowballStemmer(\"english\")\n",
    "ss_stemming_text = utils.do_stemming_words(stemmer=ss, words=wl_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_stemmer = utils.generate_word_cloud(text=ss_stemming_text, file_name='4_SnowballStemmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_cloud_stemmer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we need to compute the number of occurence for each country present in Hilary's mails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_occurrences = utils.count_countries_occurrences(ps_stemming_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_occurrences = pd.DataFrame.from_dict(countries_occurrences, orient='index')\n",
    "df_countries_occurrences.columns = ['Occurrences']\n",
    "df_countries_occurrences.sort_values('Occurrences', ascending=False, inplace=True)\n",
    "df_countries_occurrences.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utils.plot_most_quoted_countries(df_countries_occurrences,30)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice from the previous graph the number of occurence decrease quickly. <br/>\n",
    "We need to pay attention at this detail for the sentimental study where an insufficient number of occurence could change the sense of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_emails_content = df_emails[['ExtractedSubject', 'ExtractedBodyText']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental Study"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "first apply the senti_synsets function in order to get the sentimal scrore to each mail using a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = df_emails_content.apply(utils.retrieve_email_sentiment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_plot = pd.DataFrame(pd.value_counts(results['Type']))\n",
    "results_plot.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can notice from the previous graph the number of Positive value found in mails are greater compare to Neutral and Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_vader = df_emails_content.apply(utils.retrieve_email_sentiment, args=('Vader',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_vader_plot = pd.DataFrame(pd.value_counts(results_vader['Type']))\n",
    "result_vader_plot.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using vader we got a different distribution of the data. The difference come to the fact that senti_synsets function doesn't take into account the entire sentence and check the sentiments for individuals words only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countries_sentiment = utils.get_countries_sentiment(results_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_countries_sentiment = pd.DataFrame.from_dict(countries_sentiment, orient='index')\n",
    "df_countries_sentiment.columns = ['Sentiment']\n",
    "df_countries_sentiment.sort_values('Sentiment', ascending=False, inplace=True)\n",
    "df_countries_sentiment.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the two variable together Occurence/sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_data = pd.merge(df_countries_occurrences, df_countries_sentiment, how='inner', left_index=True, right_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have noticed that sentiments accorded to haiti become larger compared to other countries, we need to adjust \n",
    "# the scale of sentiments in order to obtain a result that can be easily observe\n",
    "min_max_scaler = preprocessing.MinMaxScaler((-1, 1))\n",
    "countries_data[['Sentiment']] = np.log(countries_data['Sentiment'] + abs(min(countries_data['Sentiment'])) + 10)\n",
    "countries_data['Sentiment'] = min_max_scaler.fit_transform(countries_data[['Sentiment']].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.plot_most_occurence_contry(countries_data,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The separator works only for nb_contry=20\n",
    "# The line separate the top best feeling about the \"nb_contry\" contry, and the flop worst feeling selected by\n",
    "# \"nb_contry\" contry\n",
    "utils.plot_sentiment_by_contry(countries_data,None,nb_contry=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can notice except for few country (Lativia,Serbia,Lybia), bad Feeling is associate to few occurence.\n",
    "The same things occur to the good feeling where most of them are associated to lot of occurence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = utils.SENTENCES_DETECTOR.tokenize(ps_stemming_text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "# http://christop.club/2014/05/06/using-gensim-for-lda/\n",
    "# http://stackoverflow.com/questions/15016025/how-to-print-the-lda-topics-models-from-gensim-python\n",
    "all_text_array = [[word for word in sentence.lower().split()] for sentence in sentences]\n",
    "dictionary = corpora.Dictionary(all_text_array)\n",
    "\n",
    "id2word = {}\n",
    "for word in dictionary.token2id:    \n",
    "    id2word[dictionary.token2id[word]] = word\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in all_text_array]\n",
    "lda = models.LdaModel(corpus, id2word=id2word, num_topics=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#better with karate_graph() as defined in networkx example.\n",
    "#erdos renyi don't have true community structure\n",
    "G = nx.erdos_renyi_graph(30, 0.05)\n",
    "\n",
    "#first compute the best partition\n",
    "partition = community.best_partition(G)\n",
    "\n",
    "#drawing\n",
    "size = float(len(set(partition.values())))\n",
    "pos = nx.spring_layout(G)\n",
    "count = 0.\n",
    "for com in set(partition.values()) :\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "    nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20,\n",
    "                                node_color = str(count / size))\n",
    "\n",
    "\n",
    "nx.draw_networkx_edges(G,pos, alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "links = defaultdict(lambda : defaultdict(int))\n",
    "nodes = set()\n",
    "for email in df_emails.itertuples():\n",
    "    senderId = email.SenderPersonId\n",
    "    if senderId:\n",
    "        nodes |= {int(senderId)}\n",
    "        receivers = df_email_receivers.loc[df_email_receivers['EmailId'] == email.Index]\n",
    "        for receiver in receivers.itertuples():\n",
    "            nodes |= {int(receiver.PersonId)}\n",
    "            links[int(senderId)][int(receiver.PersonId)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_links = []\n",
    "for sender, receivers in links.items():\n",
    "    for receiver, weight in receivers.items():\n",
    "        list_links.append((sender, receiver))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(list(nodes))\n",
    "G.add_edges_from(list_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nx.draw_circular(G, node_size=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
